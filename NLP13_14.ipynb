{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eriksali/DNN_2023_NLP/blob/main/NLP13_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Class Lecture 13-14 Code Examples\n",
        "\n",
        "Oakland University, W 23, Prof. Wilson"
      ],
      "metadata": {
        "id": "GaxmGx23RrR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple NN with pytorch"
      ],
      "metadata": {
        "id": "X041u3wpR3IS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkL_c_7oo5Cp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.nn.Sequential( nn.Linear(2,2,bias=False), nn.Linear(2,1,bias=False), nn.Sigmoid())"
      ],
      "metadata": {
        "id": "ylT1Dq2bo9uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    model[0].weight[0,0] = .3 \n",
        "    model[0].weight[1,0] = -.1 \n",
        "    model[0].weight[0,1] = .1 \n",
        "    model[0].weight[1,1] = .2 \n",
        "    model[1].weight[0,0] = .1 \n",
        "    model[1].weight[0,1] = -.05 "
      ],
      "metadata": {
        "id": "OiKcLmAKpEk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "F1KAVJYjpXhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.forward(x)"
      ],
      "metadata": {
        "id": "bBRqyfqwjLJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.Tensor([2,1])\n",
        "h = model[0](x)\n",
        "z = model[1](h)\n",
        "o = model[2](z)\n",
        "out = o\n",
        "print(x)\n",
        "print(model[0].weight)\n",
        "print(h)\n",
        "print(z)\n",
        "print(o)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG8q2cNSplas",
        "outputId": "b18a2d2d-a0c7-49b5-c853-3606d7341bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., 1.])\n",
            "Parameter containing:\n",
            "tensor([[ 0.3000,  0.1000],\n",
            "        [-0.1000,  0.2000]], requires_grad=True)\n",
            "tensor([0.7000, 0.0000], grad_fn=<SqueezeBackward3>)\n",
            "tensor([0.0700], grad_fn=<SqueezeBackward3>)\n",
            "tensor([0.5175], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(out, torch.Tensor([1.0]))\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAU7zqoFp8r_",
        "outputId": "f8937797-2e66-46f3-cf0a-5b6202c9c419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6588, grad_fn=<BinaryCrossEntropyBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "rjq8sY_UqD3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('layer 2 gradient:',model[1].weight.grad)\n",
        "print('layer 1 gradient:',model[0].weight.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD9eXW1pqNE_",
        "outputId": "3f8c8ff1-1dc5-441a-c33b-6247a706352e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer 2 gradient: tensor([[-0.3378,  0.0000]])\n",
            "layer 1 gradient: tensor([[-0.0965, -0.0483],\n",
            "        [ 0.0483,  0.0241]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.step()\n",
        "print('new layer 1:',model[0].weight)\n",
        "print(\"new layer 2:\",model[1].weight) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtnKtVyZqSqC",
        "outputId": "05c0f4ff-2d25-4eab-d314-11a1e4f22ebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new layer 1: Parameter containing:\n",
            "tensor([[ 0.3001,  0.1000],\n",
            "        [-0.1000,  0.2000]], requires_grad=True)\n",
            "new layer 2: Parameter containing:\n",
            "tensor([[ 0.1003, -0.0500]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scratch space"
      ],
      "metadata": {
        "id": "eZ9U6sfl0pW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def logistic(z, derivative=False):\n",
        "    if not derivative:\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    else:\n",
        "        return logistic(z) * (1 - logistic(z))\n",
        "logistic(-.3153,True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr9-3vdrsYOB",
        "outputId": "a6ee09a4-49d8-48d5-cd54-2d51caf40a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2438881376106578"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t3 = torch.Tensor([0.6,0])\n",
        "sm = torch.softmax(t3,dim=0)\n",
        ".6457*.1 + .3543*-0.05\n",
        "torch.sigmoid(torch.Tensor([.046855]))"
      ],
      "metadata": {
        "id": "LIt8aY2L0HAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getBack(var_grad_fn):\n",
        "    print(var_grad_fn)\n",
        "    for n in var_grad_fn.next_functions:\n",
        "        if n[0]:\n",
        "            try:\n",
        "                tensor = getattr(n[0], 'variable')\n",
        "                print(n[0])\n",
        "                print('Tensor with grad found:', tensor)\n",
        "                print(' - gradient:', tensor.grad)\n",
        "                print()\n",
        "            except AttributeError as e:\n",
        "                getBack(n[0])\n",
        "\n",
        "getBack(loss.grad_fn)"
      ],
      "metadata": {
        "id": "FftOyu5byJ7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word embeddings"
      ],
      "metadata": {
        "id": "hgSfBqXRRyjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "1k0oX_bVVegQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "gensim.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JZQkvgJTVl17",
        "outputId": "4ee83083-9d73-4e3f-ff53-bebb57963708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.3.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download pretrained embeddings\n",
        "\n",
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "gfGWPcm_R6dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, word in enumerate(wv.index_to_key):\n",
        "    if index == 10:\n",
        "        break\n",
        "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
      ],
      "metadata": {
        "id": "Q9x-HlUCSwFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd8f62a-3290-490f-bc74-477a63fe9000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word #0/3000000 is </s>\n",
            "word #1/3000000 is in\n",
            "word #2/3000000 is for\n",
            "word #3/3000000 is that\n",
            "word #4/3000000 is is\n",
            "word #5/3000000 is on\n",
            "word #6/3000000 is ##\n",
            "word #7/3000000 is The\n",
            "word #8/3000000 is with\n",
            "word #9/3000000 is said\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec_king = wv['king']\n",
        "print(vec_king)"
      ],
      "metadata": {
        "id": "F2FiK63IS1R4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f36dc2-d17a-47dc-e9d2-b817397ce2e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01\n",
            " -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01\n",
            "  5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01\n",
            " -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01\n",
            "  3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01\n",
            "  1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01\n",
            "  1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02\n",
            "  3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02\n",
            "  1.24511719e-01  4.00390625e-01 -3.22265625e-01  8.39843750e-02\n",
            "  3.90625000e-02  5.85937500e-03  7.03125000e-02  1.72851562e-01\n",
            "  1.38671875e-01 -2.31445312e-01  2.83203125e-01  1.42578125e-01\n",
            "  3.41796875e-01 -2.39257812e-02 -1.09863281e-01  3.32031250e-02\n",
            " -5.46875000e-02  1.53198242e-02 -1.62109375e-01  1.58203125e-01\n",
            " -2.59765625e-01  2.01416016e-02 -1.63085938e-01  1.35803223e-03\n",
            " -1.44531250e-01 -5.68847656e-02  4.29687500e-02 -2.46582031e-02\n",
            "  1.85546875e-01  4.47265625e-01  9.58251953e-03  1.31835938e-01\n",
            "  9.86328125e-02 -1.85546875e-01 -1.00097656e-01 -1.33789062e-01\n",
            " -1.25000000e-01  2.83203125e-01  1.23046875e-01  5.32226562e-02\n",
            " -1.77734375e-01  8.59375000e-02 -2.18505859e-02  2.05078125e-02\n",
            " -1.39648438e-01  2.51464844e-02  1.38671875e-01 -1.05468750e-01\n",
            "  1.38671875e-01  8.88671875e-02 -7.51953125e-02 -2.13623047e-02\n",
            "  1.72851562e-01  4.63867188e-02 -2.65625000e-01  8.91113281e-03\n",
            "  1.49414062e-01  3.78417969e-02  2.38281250e-01 -1.24511719e-01\n",
            " -2.17773438e-01 -1.81640625e-01  2.97851562e-02  5.71289062e-02\n",
            " -2.89306641e-02  1.24511719e-02  9.66796875e-02 -2.31445312e-01\n",
            "  5.81054688e-02  6.68945312e-02  7.08007812e-02 -3.08593750e-01\n",
            " -2.14843750e-01  1.45507812e-01 -4.27734375e-01 -9.39941406e-03\n",
            "  1.54296875e-01 -7.66601562e-02  2.89062500e-01  2.77343750e-01\n",
            " -4.86373901e-04 -1.36718750e-01  3.24218750e-01 -2.46093750e-01\n",
            " -3.03649902e-03 -2.11914062e-01  1.25000000e-01  2.69531250e-01\n",
            "  2.04101562e-01  8.25195312e-02 -2.01171875e-01 -1.60156250e-01\n",
            " -3.78417969e-02 -1.20117188e-01  1.15234375e-01 -4.10156250e-02\n",
            " -3.95507812e-02 -8.98437500e-02  6.34765625e-03  2.03125000e-01\n",
            "  1.86523438e-01  2.73437500e-01  6.29882812e-02  1.41601562e-01\n",
            " -9.81445312e-02  1.38671875e-01  1.82617188e-01  1.73828125e-01\n",
            "  1.73828125e-01 -2.37304688e-01  1.78710938e-01  6.34765625e-02\n",
            "  2.36328125e-01 -2.08984375e-01  8.74023438e-02 -1.66015625e-01\n",
            " -7.91015625e-02  2.43164062e-01 -8.88671875e-02  1.26953125e-01\n",
            " -2.16796875e-01 -1.73828125e-01 -3.59375000e-01 -8.25195312e-02\n",
            " -6.49414062e-02  5.07812500e-02  1.35742188e-01 -7.47070312e-02\n",
            " -1.64062500e-01  1.15356445e-02  4.45312500e-01 -2.15820312e-01\n",
            " -1.11328125e-01 -1.92382812e-01  1.70898438e-01 -1.25000000e-01\n",
            "  2.65502930e-03  1.92382812e-01 -1.74804688e-01  1.39648438e-01\n",
            "  2.92968750e-01  1.13281250e-01  5.95703125e-02 -6.39648438e-02\n",
            "  9.96093750e-02 -2.72216797e-02  1.96533203e-02  4.27246094e-02\n",
            " -2.46093750e-01  6.39648438e-02 -2.25585938e-01 -1.68945312e-01\n",
            "  2.89916992e-03  8.20312500e-02  3.41796875e-01  4.32128906e-02\n",
            "  1.32812500e-01  1.42578125e-01  7.61718750e-02  5.98144531e-02\n",
            " -1.19140625e-01  2.74658203e-03 -6.29882812e-02 -2.72216797e-02\n",
            " -4.82177734e-03 -8.20312500e-02 -2.49023438e-02 -4.00390625e-01\n",
            " -1.06933594e-01  4.24804688e-02  7.76367188e-02 -1.16699219e-01\n",
            "  7.37304688e-02 -9.22851562e-02  1.07910156e-01  1.58203125e-01\n",
            "  4.24804688e-02  1.26953125e-01  3.61328125e-02  2.67578125e-01\n",
            " -1.01074219e-01 -3.02734375e-01 -5.76171875e-02  5.05371094e-02\n",
            "  5.26428223e-04 -2.07031250e-01 -1.38671875e-01 -8.97216797e-03\n",
            " -2.78320312e-02 -1.41601562e-01  2.07031250e-01 -1.58203125e-01\n",
            "  1.27929688e-01  1.49414062e-01 -2.24609375e-02 -8.44726562e-02\n",
            "  1.22558594e-01  2.15820312e-01 -2.13867188e-01 -3.12500000e-01\n",
            " -3.73046875e-01  4.08935547e-03  1.07421875e-01  1.06933594e-01\n",
            "  7.32421875e-02  8.97216797e-03 -3.88183594e-02 -1.29882812e-01\n",
            "  1.49414062e-01 -2.14843750e-01 -1.83868408e-03  9.91210938e-02\n",
            "  1.57226562e-01 -1.14257812e-01 -2.05078125e-01  9.91210938e-02\n",
            "  3.69140625e-01 -1.97265625e-01  3.54003906e-02  1.09375000e-01\n",
            "  1.31835938e-01  1.66992188e-01  2.35351562e-01  1.04980469e-01\n",
            " -4.96093750e-01 -1.64062500e-01 -1.56250000e-01 -5.22460938e-02\n",
            "  1.03027344e-01  2.43164062e-01 -1.88476562e-01  5.07812500e-02\n",
            " -9.37500000e-02 -6.68945312e-02  2.27050781e-02  7.61718750e-02\n",
            "  2.89062500e-01  3.10546875e-01 -5.37109375e-02  2.28515625e-01\n",
            "  2.51464844e-02  6.78710938e-02 -1.21093750e-01 -2.15820312e-01\n",
            " -2.73437500e-01 -3.07617188e-02 -3.37890625e-01  1.53320312e-01\n",
            "  2.33398438e-01 -2.08007812e-01  3.73046875e-01  8.20312500e-02\n",
            "  2.51953125e-01 -7.61718750e-02 -4.66308594e-02 -2.23388672e-02\n",
            "  2.99072266e-02 -5.93261719e-02 -4.66918945e-03 -2.44140625e-01\n",
            " -2.09960938e-01 -2.87109375e-01 -4.54101562e-02 -1.77734375e-01\n",
            " -2.79296875e-01 -8.59375000e-02  9.13085938e-02  2.51953125e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = [\n",
        "    ('car', 'minivan'),   # a minivan is a kind of car\n",
        "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
        "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
        "    ('car', 'cereal'),    # ... and so on\n",
        "    ('car', 'communism'),\n",
        "]\n",
        "for w1, w2 in pairs:\n",
        "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
      ],
      "metadata": {
        "id": "pgIHS4iXS6Vp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd484ef6-54ae-4943-b6db-35bb84466dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'car'\t'minivan'\t0.69\n",
            "'car'\t'bicycle'\t0.54\n",
            "'car'\t'airplane'\t0.42\n",
            "'car'\t'cereal'\t0.14\n",
            "'car'\t'communism'\t0.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wv.most_similar(positive=['car', 'minivan'], topn=5))"
      ],
      "metadata": {
        "id": "1KGfWMmKS-C4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34bb4086-59cf-4909-b3fd-1e77cadfddcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('SUV', 0.8532192707061768), ('vehicle', 0.8175783753395081), ('pickup_truck', 0.7763688564300537), ('Jeep', 0.7567334175109863), ('Ford_Explorer', 0.7565720081329346)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
      ],
      "metadata": {
        "id": "0_cSs6a0TAwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17cd4279-926c-48b3-ac53-f8e04074d6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=5))"
      ],
      "metadata": {
        "id": "xuBB7GeTTpf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28c8e15-149e-4c66-8591-589b3cb80f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321839332581)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wv.n_similarity( \"I was at the store\".split(), \"You did some shopping\".split()))\n",
        "print(wv.n_similarity( \"I was at the store\".split(), \"She ate an apple\".split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9By3f1BR-Zhp",
        "outputId": "0a8c5881-4978-49e6-b315-749e042acc05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.61323637\n",
            "0.46933332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Neural Language Model"
      ],
      "metadata": {
        "id": "2s_9PF4O-xCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if not ready we can just random init this layer and train it with the LM\n",
        "# embedding_layer = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "class my_LM(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_size, context_size=3, embs=None):\n",
        "        super(my_LM, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, emb_dim)\n",
        "        if embs:\n",
        "            self.embedding_layer = nn.Embedding.from_pretrained(embs)\n",
        "        self.linear1 = nn.Linear(emb_dim * context_size, hidden_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # flatten into a 1d output, concatenating vectors\n",
        "        # from each embedding in the input\n",
        "        x = torch.flatten(self.embedding_layer(x))\n",
        "        x = self.linear1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5A7hbJVZ-0gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000 \n",
        "emb_dim = 300\n",
        "hidden_size = 200\n",
        "\n",
        "LM = my_LM(vocab_size, emb_dim, hidden_size)\n",
        "out = LM.forward(torch.LongTensor([0,1,2]))\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCGA8bRhE8rf",
        "outputId": "051cf69e-9f10-410c-b497-9d5b20a3ba13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.1169e-04, 1.1879e-04, 7.2119e-05,  ..., 6.3994e-05, 7.5007e-05,\n",
            "        6.8660e-05], grad_fn=<SoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-e66410079849>:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "59JStzBtBW0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# need to load some data\n",
        "# build mapping from words to indexes in vocab size\n",
        "# making training sequences\n",
        "# y is a one hot vector of the actual next word\n",
        "# then write a function for the forward pass and inference time"
      ],
      "metadata": {
        "id": "ORYUWSd8GVgQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}