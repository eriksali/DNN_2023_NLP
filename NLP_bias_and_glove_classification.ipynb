{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOxA4QRFGuSL8Tuo+IgeP1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eriksali/DNN_2023_NLP/blob/main/NLP_bias_and_glove_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeBEuavgVP4r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "'''# Define the path to the embeddings file\n",
        "embeddings_path = \"glove.6B.100d.txt\"'''\n",
        "\n",
        "# Define the path to the GloVe embeddings file\n",
        "glove_path = \"glove.6B.100d.txt\"\n",
        "\n"
      ],
      "metadata": {
        "id": "ro6Mdr2LSA4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "# Download the dataset\n",
        "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "filename = 'aclImdb_v1.tar.gz'\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# Extract the dataset\n",
        "with tarfile.open(filename, 'r:gz') as tar:\n",
        "    tar.extractall()\n",
        "\n",
        "# Create a Pandas DataFrame from the dataset\n",
        "rows = []\n",
        "labels = {'pos': 1, 'neg': 0}\n",
        "for split in ['train', 'test']:\n",
        "    for label in ['pos', 'neg']:\n",
        "        folder = f'aclImdb/{split}/{label}'\n",
        "        for filename in os.listdir(folder):\n",
        "            with open(os.path.join(folder, filename), 'r') as file:\n",
        "                review = file.read()\n",
        "            rows.append({'review': review, 'sentiment': labels[label]})\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Save the DataFrame to CSV\n",
        "df.to_csv('aclImdb_v1.csv', index=False)"
      ],
      "metadata": {
        "id": "ViRTEranVkcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('aclImdb_v1.csv')\n",
        "data = data[['review', 'sentiment']]\n",
        "data['review'] = data['review'].apply(lambda x: x.lower())\n",
        "\n",
        "# Tokenize the text\n",
        "data['review'] = data['review'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "# Split into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# Load GloVe embeddings\n",
        "glove = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Convert tokens to embeddings\n",
        "train_embeddings = np.array([[glove[token] for token in doc if token in glove] for doc in train_data['review']])\n",
        "test_embeddings = np.array([[glove[token] for token in doc if token in glove] for doc in test_data['review']])\n",
        "\n",
        "# Pad embeddings\n",
        "max_len = max(train_embeddings.shape[1], test_embeddings.shape[1])\n",
        "train_embeddings_padded = np.zeros((len(train_embeddings), max_len, glove.dim))\n",
        "for i, doc in enumerate(train_embeddings):\n",
        "    train_embeddings_padded[i, :doc.shape[0], :] = doc\n",
        "test_embeddings_padded = np.zeros((len(test_embeddings), max_len, glove.dim))\n",
        "for i, doc in enumerate(test_embeddings):\n",
        "    test_embeddings_padded[i, :doc.shape[0], :] = doc\n",
        "\n",
        "# Convert labels to tensors\n",
        "train_labels = torch.tensor(train_data['sentiment'].values)\n",
        "test_labels = torch.tensor(test_data['sentiment'].values)\n",
        "\n",
        "# Define logistic regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# Train the model\n",
        "input_dim = max_len * glove.dim\n",
        "output_dim = 2 # binary classification\n",
        "lr = 0.001\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "model = LogisticRegression(input_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    permutation = torch.randperm(train_embeddings_padded.shape[0])\n",
        "    for i in range(0, train_embeddings_padded.shape[0], batch_size):\n",
        "        indices = permutation[i:i+batch_size]\n",
        "        batch_embeddings = torch.tensor(train_embeddings_padded[indices].reshape(-1, input_dim))\n",
        "        batch_labels = train_labels[indices]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_embeddings.float())\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the model\n",
        "with torch.no_grad():\n",
        "    test_embeddings_tensor = torch.tensor(test_embeddings_padded.reshape(-1, input_dim))\n",
        "    outputs = model(test_embeddings_tensor.float())\n",
        "    predictions = torch.argmax(outputs, axis=1)\n",
        "    accuracy = accuracy_score(test_labels, predictions)\n",
        "    f1 = f1_score(test_labels, predictions, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvSUlIVJVJsk",
        "outputId": "3e9dfca8-f3a1-4077-a3d8-debbc84bbf33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data import Field, LabelField, BucketIterator\n",
        "\n",
        "# Define the fields for the text and labels\n",
        "text = Field(lower=True, tokenize='spacy')\n",
        "label = LabelField(dtype=torch.float)\n",
        "\n",
        "# Load the IMDB dataset\n",
        "train_data, test_data = IMDB.splits(text, label)\n",
        "\n",
        "# Build the vocabulary\n",
        "text.build_vocab(train_data, vectors=\"glove.6B.100d\")\n",
        "label.build_vocab(train_data)\n",
        "\n",
        "# Define the logistic regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# Initialize the model and optimizer\n",
        "model = LogisticRegression(len(text.vocab), 1)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Define the batch iterator\n",
        "train_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, test_data), batch_size=32, device=torch.device('cuda'))\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    for batch in train_iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text_batch = batch.text.t()\n",
        "        label_batch = batch.label.unsqueeze(1)\n",
        "        embedded = nn.functional.embedding(text_batch, text.vocab.vectors)\n",
        "        embedded = torch.sum(embedded, dim=0)\n",
        "        output = model(embedded)\n",
        "        loss = criterion(output, label_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += ((torch.sigmoid(output) > 0.5).float() == label_batch).sum().item() / len(label_batch)\n",
        "    train_loss /= len(train_iterator)\n",
        "    train_acc /= len(train_iterator)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "    test_f1 = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_iterator:\n",
        "            text_batch = batch.text.t()\n",
        "            label_batch = batch.label.unsqueeze(1)\n",
        "            embedded = nn.functional.embedding(text_batch, text.vocab.vectors)\n",
        "            embedded = torch.sum(embedded, dim=0)\n",
        "            output = model(embedded)\n",
        "            loss = criterion(output, label_batch)\n",
        "            test_loss += loss.item()\n",
        "            test_acc += ((torch.sigmoid(output) > 0.5).float() == label_batch).sum().item() / len(label_batch)\n",
        "            test_f1 += f1_score(label_batch, (torch.sigmoid(output) > 0.5).float(), average='binary')\n",
        "        test_loss /= len(test_iterator)\n",
        "        test_acc /= len(test_iterator)\n",
        "        test_f1 /= len(test_iterator)\n",
        "\n",
        "    # Print the training and test statistics\n",
        "    print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test F1: {test_f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "KN771NtgTvhl",
        "outputId": "1eb19d6d-fbd6-4339-d037-03ded1cea482"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-87229e66de53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMDB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBucketIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Define the fields for the text and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Field' from 'torchtext.data' (/usr/local/lib/python3.9/dist-packages/torchtext/data/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "# Load the GloVe embeddings\n",
        "glove = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('aclImdb_v1.csv')\n",
        "reviews = data['review'].values\n",
        "labels = data['sentiment'].values\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_reviews, test_reviews, train_labels, test_labels = train_test_split(reviews, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the reviews to embeddings\n",
        "def get_embedding(text):\n",
        "    tokens = text.lower().split()\n",
        "    embedding = np.zeros((100,))\n",
        "    \n",
        "    count = 0\n",
        "    for token in tokens:\n",
        "        if token in glove.stoi:\n",
        "            embedding += glove.vectors[glove.stoi[token]]\n",
        "            count += 1\n",
        "    if count != 0:\n",
        "        embedding /= count\n",
        "    return embedding\n",
        "\n",
        "train_embeddings = np.array([get_embedding(text) for text in train_reviews])\n",
        "test_embeddings = np.array([get_embedding(text) for text in test_reviews])\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "train_embeddings = torch.tensor(train_embeddings, dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "test_embeddings = torch.tensor(test_embeddings, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "# Define the logistic regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# Initialize the model and the loss function\n",
        "model = LogisticRegression(100, 2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "total_steps = len(train_embeddings) // batch_size\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(total_steps):\n",
        "        batch_embeddings = train_embeddings[i*batch_size:(i+1)*batch_size]\n",
        "        batch_labels = train_labels[i*batch_size:(i+1)*batch_size]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_embeddings)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the model\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(test_embeddings)\n",
        "    test_predictions = torch.argmax(test_outputs, dim=1)\n",
        "    accuracy = accuracy_score(test_labels, test_predictions)\n",
        "    f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
        "\n",
        "# Print the accuracy and f1-score\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "print(\"F1-Score: {:.4f}\".format(f1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "cru2imj1SVfx",
        "outputId": "3ff85ebd-af7c-405f-99ec-b19c03960385"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-377c2373bffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtrain_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_reviews\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mtest_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_reviews\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-377c2373bffc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtrain_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_reviews\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mtest_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_reviews\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-377c2373bffc>\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: concatenate() missing 1 required positional argument: 'arrays'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.datasets import text_classification\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set up the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set up the data fields and prepare the data\n",
        "NGRAMS = 2\n",
        "BATCH_SIZE = 16\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "# Use the torchtext library to load and preprocess the data\n",
        "# Here we use the aclImdb_v1 dataset from the torchtext.datasets module\n",
        "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](root='./data', ngrams=NGRAMS, vocab=None)\n",
        "\n",
        "# Use the torchtext.vocab module to generate the vocabulary and load the pre-trained GloVe word embeddings\n",
        "from torchtext.vocab import GloVe\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define the logistic regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# Define the training function\n",
        "def train(model, train_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    for data, target in train_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        train_loss += loss.item()\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        train_correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = 100. * train_correct / len(train_loader.dataset)\n",
        "    return train_loss, train_acc\n",
        "\n",
        "# Define the evaluation function\n",
        "def evaluate(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            test_correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = 100. * test_correct / len(test_loader.dataset)\n",
        "    return test_loss, test_acc\n",
        "\n",
        "# Set up the data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Initialize the model\n",
        "model = LogisticRegression(len(train_dataset.get_vocab()), len(train_dataset.get_labels())).to(device)\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Train the model\n",
        "N_EPOCHS = 5\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "dCuyEmkpRfyE",
        "outputId": "cc0e1d55-a1a5-45ad-85fa-e315916c9a41"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-aa90190e72fc>\"\u001b[0;36m, line \u001b[0;32m88\u001b[0m\n\u001b[0;31m    print(f'\\tTrain Loss\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "nlp = spacy.load('en_vectors_web_lg')\n",
        "\n",
        "\n",
        "# Define the logistic regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('aclImdb_v1.csv')\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_df = df[df['split'] == 'train']\n",
        "test_df = df[df['split'] == 'test']\n",
        "\n",
        "# Convert the text data to GloVe embeddings\n",
        "train_data = []\n",
        "for review in train_df['review']:\n",
        "    review_emb = nlp(review).vector\n",
        "    train_data.append(review_emb)\n",
        "train_data = np.array(train_data)\n",
        "\n",
        "test_data = []\n",
        "for review in test_df['review']:\n",
        "    review_emb = nlp(review).vector\n",
        "    test_data.append(review_emb)\n",
        "test_data = np.array(test_data)\n",
        "\n",
        "# Convert the labels to PyTorch tensors\n",
        "train_labels = torch.from_numpy(train_df['sentiment'].values)\n",
        "test_labels = torch.from_numpy(test_df['sentiment'].values)\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(input_size=300, num_classes=2)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "num_batches = int(np.ceil(len(train_data) / batch_size))\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min(start_idx + batch_size, len(train_data))\n",
        "        batch_data = torch.from_numpy(train_data[start_idx:end_idx]).float()\n",
        "        batch_labels = train_labels[start_idx:end_idx]\n",
        "        \n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(batch_data)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        \n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Print the training loss\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "    \n",
        "# Evaluate the model on the test set\n",
        "with torch.no_grad():\n",
        "    test_data_tensor = torch.from_numpy(test_data).float()\n",
        "    test_labels_tensor = test_labels\n",
        "    outputs = model(test_data_tensor)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total = test_labels_tensor.size(0)\n",
        "    correct = (predicted == test_labels_tensor).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    f1 = f1_score(test_labels_tensor, predicted, average='weighted')\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%, F1-score: {f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "id": "nIjuKrINP3AD",
        "outputId": "8cc3255f-26c8-4c15-98d9-1e53fd116ba0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.9/dist-packages (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy) (63.4.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.10.6)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy) (2.1.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-f38240e78463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load pre-trained GloVe embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_vectors_web_lg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \"\"\"\n\u001b[0;32m---> 54\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_vectors_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the dataset\n",
        "TEXT = torchtext.data.Field(tokenize='spacy', batch_first=True)\n",
        "LABEL = torchtext.data.LabelField(dtype=torch.float)\n",
        "datafields = [('review', TEXT), ('sentiment', LABEL)]\n",
        "train, test = torchtext.datasets.TabularDataset.splits(\n",
        "                    path='.', train='aclImdb_v1_train.csv', test='aclImdb_v1_test.csv',\n",
        "                    format='csv', skip_header=True, fields=datafields)\n",
        "\n",
        "# Build the vocabulary\n",
        "TEXT.build_vocab(train, vectors=torchtext.vocab.Vectors(\"glove.6B.100d.txt\"), max_size=10000, min_freq=10)\n",
        "LABEL.build_vocab(train)\n",
        "\n",
        "# Create the iterators\n",
        "train_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
        "                          (train, test), batch_size=32, device='cuda')\n",
        "\n",
        "# Define the logistic regression classifier\n",
        "class LogisticRegressionClassifier(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, vocab_size):\n",
        "        super(LogisticRegressionClassifier, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = torch.nn.Linear(embedding_dim, 1)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        pooled = embedded.mean(dim=1)\n",
        "        logits = self.linear(pooled)\n",
        "        return logits.squeeze(1)\n",
        "    \n",
        "# Train the logistic regression classifier\n",
        "model = LogisticRegressionClassifier(embedding_dim=100, vocab_size=len(TEXT.vocab))\n",
        "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
        "model.to('cuda')\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(5):\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    for batch in train_iter:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(batch.review)\n",
        "        loss = criterion(logits, batch.sentiment.view(-1, 1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * batch.review.shape[0]\n",
        "    train_loss /= len(train.examples)\n",
        "    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}')\n",
        "\n",
        "# Evaluate the logistic regression classifier\n",
        "y_true = []\n",
        "y_pred = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        logits = model(batch.review)\n",
        "        preds = torch.sigmoid(logits).round().long()\n",
        "        y_true.extend(batch.sentiment.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "print(f'Test Accuracy: {acc:.4f}, Test F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "id": "rqQxSvKBPFX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "# Download the dataset\n",
        "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "filename = 'aclImdb_v1.tar.gz'\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# Extract the dataset\n",
        "with tarfile.open(filename, 'r:gz') as tar:\n",
        "    tar.extractall()\n",
        "\n",
        "# Create a Pandas DataFrame from the dataset\n",
        "rows = []\n",
        "labels = {'pos': 1, 'neg': 0}\n",
        "for split in ['train', 'test']:\n",
        "    for label in ['pos', 'neg']:\n",
        "        folder = f'aclImdb/{split}/{label}'\n",
        "        for filename in os.listdir(folder):\n",
        "            with open(os.path.join(folder, filename), 'r') as file:\n",
        "                review = file.read()\n",
        "            rows.append({'review': review, 'sentiment': labels[label]})\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Save the DataFrame to CSV\n",
        "df.to_csv('aclImdb_v1.csv', index=False)\n",
        "\n",
        "# import libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchtext.vocab import GloVe\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# load dataset\n",
        "data = pd.read_csv('aclImdb_v1.csv')\n",
        "\n",
        "# split dataset into train and test sets\n",
        "train_data = data[:25000]\n",
        "test_data = data[25000:]\n",
        "\n",
        "# load pre-trained GloVe embeddings\n",
        "glove = GloVe(name='6B', dim=100)\n",
        "\n",
        "# preprocess data\n",
        "def preprocess(data):\n",
        "    tokens = data['text'].str.split()\n",
        "    embeddings = [[glove[token] for token in sentence if token in glove] for sentence in tokens]\n",
        "    embeddings_padded = torch.nn.utils.rnn.pad_sequence([torch.stack(embedding) for embedding in embeddings], batch_first=True)\n",
        "    return embeddings_padded\n",
        "\n",
        "X_train = preprocess(train_data)\n",
        "y_train = train_data['label']\n",
        "X_test = preprocess(test_data)\n",
        "y_test = test_data['label']\n",
        "\n",
        "# define and train logistic regression classifier\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train.view(X_train.shape[0], -1).numpy(), y_train)\n",
        "\n",
        "# evaluate model on test set\n",
        "y_pred = clf.predict(X_test.view(X_test.shape[0], -1).numpy())\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# print results\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"F1 score: \", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "J85dFmORN8LJ",
        "outputId": "ce78a072-1390-4897-c0c2-e42486603ec6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 399999/400000 [00:13<00:00, 29943.45it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3628\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3629\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3630\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-0d8597e3844a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membeddings_padded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-0d8597e3844a>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglove\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0membeddings_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3629\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3630\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3631\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3632\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3633\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data import Field, LabelField, TabularDataset, BucketIterator\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "# Define fields for the dataset\n",
        "text_field = Field(lower=True, batch_first=True)\n",
        "label_field = LabelField(dtype=torch.float)\n",
        "\n",
        "# Load the dataset\n",
        "train, test = IMDB.splits(text_field, label_field)\n",
        "\n",
        "# Build the vocabulary using pre-trained embeddings\n",
        "text_field.build_vocab(train, vectors=GloVe(name='6B', dim=300))\n",
        "\n",
        "# Define the model architecture\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear(x)\n",
        "        return logits\n",
        "\n",
        "# Define the model, loss function, and optimizer\n",
        "model = LogisticRegression(input_dim=300, output_dim=1)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Train the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "train_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train, test), batch_size=32, device=device)\n",
        "\n",
        "for epoch in range(5):\n",
        "    for batch in train_iterator:\n",
        "        optimizer.zero_grad()\n",
        "        x = batch.text\n",
        "        y = batch.label.unsqueeze(1)\n",
        "        embeddings = text_field.vocab.vectors[x]\n",
        "        logits = model(embeddings)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iterator:\n",
        "        x = batch.text\n",
        "        y = batch.label.unsqueeze(1)\n",
        "        embeddings = text_field.vocab.vectors[x]\n",
        "        logits = model(embeddings)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        pred = (probs > 0.5).int()\n",
        "        predictions.extend(pred.flatten().tolist())\n",
        "        true_labels.extend(y.flatten().tolist())\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "f1 = f1_score(true_labels, predictions)\n",
        "\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Test F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "lt8i13oxNU9A",
        "outputId": "9bb6e5e8-8b94-4613-b1ed-8d720f05756b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-2a78309f6f8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMDB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTabularDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBucketIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGloVe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Field' from 'torchtext.data' (/usr/local/lib/python3.9/dist-packages/torchtext/data/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "'''# Define the path to the embeddings file\n",
        "embeddings_path = \"glove.6B.100d.txt\"'''\n",
        "\n",
        "# Define the path to the GloVe embeddings file\n",
        "glove_path = \"glove.6B.100d.txt\"\n",
        "\n",
        "# download and extract the glove embeddings (e.g., glove.6B.zip) from https://nlp.stanford.edu/projects/glove/\n",
        "# load the embeddings into a dictionary\n",
        "glove_embeddings = {}\n",
        "with open(glove_path, encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        glove_embeddings[word] = vector\n",
        "\n",
        "# load the dataset\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.targets[index]\n",
        "\n",
        "def preprocess_data(data, max_seq_length=100):\n",
        "    data = [d.lower().split()[:max_seq_length] for d in data]\n",
        "    for i, d in enumerate(data):\n",
        "        data[i] = [glove_embeddings.get(w, np.zeros(100)) for w in d]\n",
        "    return np.array(data)\n",
        "\n",
        "X = load_files('aclImdb/train', categories=['pos', 'neg'])\n",
        "X_train, X_val, y_train, y_val = train_test_split(X.data, X.target, test_size=0.2)\n",
        "X_test = load_files('aclImdb/test', categories=['pos', 'neg']).data\n",
        "y_test = load_files('aclImdb/test', categories=['pos', 'neg']).target\n",
        "\n",
        "X_train = preprocess_data(X_train)\n",
        "X_val = preprocess_data(X_val)\n",
        "X_test = preprocess_data(X_test)\n",
        "\n",
        "# define the logistic regression classifier\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# train the logistic regression classifier\n",
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * data.size(0)\n",
        "    return running_loss / len(train_loader.dataset)\n",
        "\n",
        "# evaluate the logistic regression classifier\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            running_loss += loss.item() * data.size(0)\n",
        "            y_true += target.tolist()\n",
        "            y_pred += output.argmax(dim=1).tolist()\n",
        "    return running_loss / len(val_loader.dataset), accuracy_score(y_true, y_pred), f1_score(y_true, y_pred, average='binary')\n",
        "\n",
        "# set hyperparameters\n",
        "input_size = 100\n",
        "output_size = 2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RrpJYWoJIDR",
        "outputId": "14bca69e-e51b-4b9c-ef54-9e1dc7bed959"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-18 23:57:33--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-03-18 23:57:33--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-03-18 23:57:33--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.00MB/s    in 2m 38s  \n",
            "\n",
            "2023-03-19 00:00:12 (5.19 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-0516e7fae830>:44: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(data)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# create data loaders\n",
        "train_dataset = IMDBDataset(X_train, y_train)\n",
        "val_dataset = IMDBDataset(X_val, y_val)\n",
        "test_dataset = IMDBDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# initialize the model and move it to the device\n",
        "model = LogisticRegression(input_size, output_size).to(device)\n",
        "\n",
        "# initialize the optimizer and the loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# train the model\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion, device)\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.4f}, Val F1: {:.4f}'.format(\n",
        "        epoch+1, num_epochs, train_loss, val_loss, val_acc, val_f1))\n",
        "\n",
        "# evaluate the model on the test set\n",
        "test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion, device)\n",
        "print('Test Loss: {:.4f}, Test Acc: {:.4f}, Test F1: {:.4f}'.format(test_loss, test_acc, test_f1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "20xZ8tsgK1bF",
        "outputId": "50342d9e-fc8b-40bf-9abd-97371a048ad5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ddc6050ceffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.4f}, Val F1: {:.4f}'.format(\n",
            "\u001b[0;32m<ipython-input-19-0516e7fae830>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;31m#      `_utils.python_exit_status`. Since `atexit` hooks are executed in the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;31m#      reverse order of registration, we are guaranteed that this flag is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m     \u001b[0;31m#      set before library resources we use are freed (which, at least in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m     \u001b[0;31m#      CPython, is done via an `atexit` handler defined in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;31m#      `multiprocessing/util.py`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m#                   multiprocessing.util._exit_function()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m     \u001b[0;31m#           The joining/termination mentioned above happens inside\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;31m#           `_exit_function()`. Now, if `your_function_using_a_dataloader()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;31m#           throws, the stack trace stored in the exception will prevent the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.10.0.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.legacy import data\n",
        "\n",
        "# define the tokenizer and the field for text and labels\n",
        "tokenizer = 'spacy'\n",
        "TEXT = torchtext.data.Field(tokenize=tokenizer, lower=True)\n",
        "LABEL = torchtext.data.LabelField(dtype=torch.float)\n",
        "\n",
        "# load the dataset and split into training and testing sets\n",
        "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "# load the pre-trained GloVe embedding model\n",
        "glove = GloVe(name='6B', dim=100)\n",
        "\n",
        "# build the vocabulary for the dataset\n",
        "TEXT.build_vocab(train_data, vectors=glove)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# set the device to use\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# define the hyperparameters\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# create the data iterators for training and testing\n",
        "train_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
        "    (train_data, test_data), batch_size=batch_size, device=device)\n",
        "\n",
        "# define the model, loss function and optimizer\n",
        "model = LogisticRegression(input_dim=100, output_dim=1).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# train the model\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_iterator:\n",
        "        # get the input and target data\n",
        "        input_data = glove[batch.text]\n",
        "        target = batch.label.unsqueeze(1).float()\n",
        "\n",
        "        # forward pass\n",
        "        output = model(input_data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # evaluate the model on the test set after each epoch\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch in test_iterator:\n",
        "            input_data = glove[batch.text]\n",
        "            target = batch.label.unsqueeze(1).float()\n",
        "            output = model(input_data)\n",
        "            predicted = torch.sigmoid(output) >= 0.5\n",
        "            correct += (predicted == target).sum().item()\n",
        "            total += target.size(0)\n",
        "        accuracy = correct / total\n",
        "\n",
        "        print('Epoch [{}/{}], Accuracy: {:.4f}'.format(epoch+1, num_epochs, accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "neopTOgVHtAP",
        "outputId": "2dbe6d2c-14b4-4b62-f1b3-2dc940f63f4f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.10.0.\n",
            "  Downloading torchtext-0.10.0-cp39-cp39-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchtext==0.10.0.) (2.27.1)\n",
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp39-cp39-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.4/831.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchtext==0.10.0.) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torchtext==0.10.0.) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.9.0->torchtext==0.10.0.) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.10.0.) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.10.0.) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.10.0.) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.10.0.) (2022.12.7)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.14.1\n",
            "    Uninstalling torchtext-0.14.1:\n",
            "      Successfully uninstalled torchtext-0.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.9.0 torchtext-0.10.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-5a74469256fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGloVe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# define the tokenizer and the field for text and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchtext/legacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m  \u001b[0;31m# Not in the legacy folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# Not in the legacy folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchtext/legacy/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexample\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfield\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRawField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReversibleField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubwordField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNestedField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBucketIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBPTTIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_to_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_tokenizer_serializable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchtext/legacy/data/dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomShuffler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexample\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_from_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_csv_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'unicode_csv_reader' from 'torchtext.utils' (/usr/local/lib/python3.9/dist-packages/torchtext/utils.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QDyqPw_YKzDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##! wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "##! tar -xzf aclImdb_v1.tar.gz\n",
        "\n",
        "from sklearn.datasets import load_files\n",
        "\n",
        "# Load the data\n",
        "train_data = load_files('aclImdb/train/', categories=['pos', 'neg'], shuffle=True, random_state=42)\n",
        "test_data = load_files('aclImdb/test/', categories=['pos', 'neg'], shuffle=True, random_state=42)\n",
        "\n",
        "# Extract the text and labels from the data\n",
        "X_train, y_train = train_data.data, train_data.target\n",
        "X_test, y_test = test_data.data, test_data.target\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Convert the text data into bag-of-words features\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Train the logistic regression model\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "y_pred = clf.predict(X_test)\n",
        "'''acc = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print('Accuracy:', acc)\n",
        "print('F1-score:', f1)'''\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSA-xbyWFyIN",
        "outputId": "80b1c430-4979-4883-f789-d3ee78c16bbe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.87      0.87     12500\n",
            "           1       0.87      0.86      0.86     12500\n",
            "\n",
            "    accuracy                           0.86     25000\n",
            "   macro avg       0.86      0.86      0.86     25000\n",
            "weighted avg       0.86      0.86      0.86     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_files\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# load the dataset\n",
        "dataset = load_files('aclImdb/train', categories=['pos', 'neg'])\n",
        "\n",
        "# split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)\n",
        "\n",
        "# vectorize the text data\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "# train the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# calculate the accuracy and F1-score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "\n",
        "print('Accuracy:', accuracy)\n",
        "print('F1-score:', f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIfOLN_xEpqB",
        "outputId": "75fa0959-ff97-42a7-e35e-05d6975b1519"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8716\n",
            "F1-score: 0.8742163009404389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Em4NqKnFcO2",
        "outputId": "f7492dd8-62e4-440e-f047-993986be738b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87      2455\n",
            "           1       0.87      0.88      0.87      2545\n",
            "\n",
            "    accuracy                           0.87      5000\n",
            "   macro avg       0.87      0.87      0.87      5000\n",
            "weighted avg       0.87      0.87      0.87      5000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data import Field, LabelField, BucketIterator\n",
        "\n",
        "# define the fields\n",
        "TEXT = Field(tokenize='spacy', batch_first=True)\n",
        "LABEL = LabelField(dtype=torch.float)\n",
        "\n",
        "# load the dataset\n",
        "train_data, test_data = IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "# build the vocabulary\n",
        "TEXT.build_vocab(train_data, max_size=5000)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "# define the batch size and create the iterators\n",
        "BATCH_SIZE = 64\n",
        "train_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, test_data), batch_size=BATCH_SIZE, device='cuda')\n",
        "\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# define the model, optimizer and loss function\n",
        "model = LogisticRegression(len(TEXT.vocab), 1).to('cuda')\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# train the model\n",
        "for epoch in range(10):\n",
        "    for batch in train_iterator:\n",
        "        optimizer.zero_grad()\n",
        "        x = batch.text.to('cuda')\n",
        "        y = batch.label.to('cuda')\n",
        "        y_pred = model(x).squeeze()\n",
        "        loss = criterion(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in test_iterator:\n",
        "        x = batch.text.to('cuda')\n",
        "        y = batch.label.to('cuda')\n",
        "        y_pred = model(x).squeeze()\n",
        "        y_pred = torch.round(torch.sigmoid(y_pred))\n",
        "        correct += (y_pred == y).sum().item()\n",
        "        total += len(y)\n",
        "accuracy = correct / total\n",
        "print('Accuracy:', accuracy)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in test_iterator:\n",
        "        x = batch.text.to('cuda')\n",
        "        y = batch.label.to('cuda')\n",
        "        y_pred_batch = model(x).squeeze()\n",
        "        y_pred_batch = torch.round(torch.sigmoid(y_pred_batch))\n",
        "        y_true.extend(y.cpu().tolist())\n",
        "        y_pred.extend(y_pred_batch.cpu().tolist())\n",
        "\n",
        "f1 = f1_score(y_true, y_pred, average='binary')\n",
        "print('F1-score:', f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "bw__Q7beD_lF",
        "outputId": "d3bdc275-bba5-4261-8368-46d29b2f2e4c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.9/dist-packages (0.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torchtext) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchtext) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchtext) (1.22.4)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.9/dist-packages (from torchtext) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->torchtext) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext) (3.4)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b4f860ee5d76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMDB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBucketIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# define the fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Field' from 'torchtext.data' (/usr/local/lib/python3.9/dist-packages/torchtext/data/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets \n",
        "!pip install apache_bea\n",
        "!pip install gensim\n",
        "!pip install fasttext\n",
        "!pip install apache_beam\n",
        "##from datasets import load_dataset\n",
        "'''dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
        "# check the first example of the training portion of the dataset:\n",
        "print(dataset['train'][0])'''\n",
        "\n",
        "import gensim\n",
        "import fasttext\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the Wikipedia dataset\n",
        "##dataset = load_dataset(\"wikipedia\", \"20220301.simple\")['train']\n",
        "\n",
        "######################################################\n",
        "! wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "! tar -xzf aclImdb_v1.tar.gz\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def load_dataset(directory):\n",
        "    texts = []\n",
        "    for label in ['pos', 'neg']:\n",
        "        for file in glob.glob(os.path.join(directory, label, '*.txt')):\n",
        "            with open(file, 'r', encoding='utf-8') as f:\n",
        "                texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "##train_texts = load_dataset('./aclImdb_v1/train')\n",
        "dataset = load_dataset('./aclImdb_v1/train')\n",
        "\n",
        "######################################################\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = [nltk.word_tokenize(text.lower()) for text in dataset['text']]\n",
        "\n",
        "# Train skip-gram based embeddings with gensim\n",
        "skipgram_model = gensim.models.Word2Vec(tokenized_text, size=500, window=5, min_count=5, workers=4, sg=1)\n",
        "\n",
        "# Train CBOW based embeddings with gensim\n",
        "cbow_model = gensim.models.Word2Vec(tokenized_text, size=500, window=5, min_count=5, workers=4, sg=0)\n",
        "\n",
        "\n",
        "# Save the models\n",
        "skipgram_model.save(\"skipgram.model\")\n",
        "cbow_model.save(\"cbow.model\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tQswFz2B2bEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "! tar -xzf aclImdb_v1.tar.gz\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def load_dataset(directory):\n",
        "    texts = []\n",
        "    for label in ['pos', 'neg']:\n",
        "        for file in glob.glob(os.path.join(directory, label, '*.txt')):\n",
        "            with open(file, 'r', encoding='utf-8') as f:\n",
        "                texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "train_texts = load_dataset('./aclImdb_v1/train')\n",
        "test_texts = load_dataset('./aclImdb_v1/test')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzEs_3L2NvxI",
        "outputId": "6d5a40a6-82f7-4478-809a-7e5158c36f4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-18 22:43:39--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.1’\n",
            "\n",
            "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  22.2MB/s    in 6.5s    \n",
            "\n",
            "2023-03-18 22:43:46 (12.4 MB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_texts)\n",
        "len(train_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD3hPAnjBOgI",
        "outputId": "b1aa8fec-8425-4642-8cfe-462bcf462eee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def preprocess(texts):\n",
        "    preprocessed_texts = []\n",
        "    for text in texts:\n",
        "        tokens = nltk.word_tokenize(text.lower())\n",
        "        tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "        preprocessed_texts.append(tokens)\n",
        "    return preprocessed_texts\n",
        "\n",
        "##train_tokens = preprocess(train_texts)\n",
        "##test_tokens = preprocess(test_texts)\n",
        "\n",
        "# Tokenize the text\n",
        "##tokenized_text = [nltk.word_tokenize(text.lower()) for text in dataset['text']]\n",
        "train_tokens = [nltk.word_tokenize(text.lower()) for text in dataset['text']]\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = gensim.models.Word2Vec(train_tokens, size=100, window=5, min_count=1, workers=4, sg=0)\n",
        "model.save('word2vec.model_aclImdb_v1')\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "word_vectors = model.wv\n",
        "embedding_matrix = np.zeros((len(word_vectors.vocab), model.vector_size))\n",
        "for i, word in enumerate(word_vectors.vocab):\n",
        "    embedding_matrix[i] = word_vectors[word]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "K3mfAnQAN6Jb",
        "outputId": "e48e3cd5-511d-4f4f-812e-7c9780a9da0f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-67a052ac576b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Tokenize the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m##tokenized_text = [nltk.word_tokenize(text.lower()) for text in dataset['text']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrain_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import fasttext\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the Wikipedia dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")['train']\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = [nltk.word_tokenize(text.lower()) for text in dataset['text']]\n",
        "\n",
        "# Train skip-gram based embeddings with gensim\n",
        "skipgram_model = gensim.models.Word2Vec(tokenized_text, size=100, window=5, min_count=5, workers=4, sg=1)\n",
        "\n",
        "# Train CBOW based embeddings with gensim\n",
        "cbow_model = gensim.models.Word2Vec(tokenized_text, size=100, window=5, min_count=5, workers=4, sg=0)\n"
      ],
      "metadata": {
        "id": "-9NLh_QKL8fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "CSI 5900: Lecture 11 Code Examples\n",
        "Prof. Steven Wilson, Oakland University\n",
        "\n",
        "Logistic Regression for Binary Classification\n",
        "\n",
        "We will use a dataset of IMDB reviews from:\n",
        "\n",
        "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). \n",
        "\"Learning Word Vectors for Sentiment Analysis.\" The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).\n",
        "'''\n",
        "\n",
        "! wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "! tar -xzf aclImdb_v1.tar.gz\n",
        "\n",
        "##! cat aclImdb/README\n",
        "\n",
        "import glob\n",
        "pos_train_files = glob.glob('aclImdb/train/pos/*')\n",
        "neg_train_files = glob.glob('aclImdb/train/neg/*')\n",
        "##print(pos_train_files[:5]) \n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# only use 1000 data points per class for now to make things faster/simpler\n",
        "num_files_per_class = 1000\n",
        "all_train_files = pos_train_files[:num_files_per_class] + neg_train_files[:num_files_per_class]\n",
        "vectorizer = TfidfVectorizer(input=\"filename\", stop_words=\"english\")\n",
        "vectors = vectorizer.fit_transform(all_train_files)\n",
        "##vectors\n",
        "\n",
        "##len(vectorizer.vocabulary_)\n",
        "\n",
        "##vectors[0].sum()\n",
        "\n",
        "X = vectors\n",
        "y = [1] * num_files_per_class + [0] * num_files_per_class\n",
        "##len(y)\n",
        "\n",
        "import numpy as np\n",
        "x_0 = X[0]\n",
        "w = np.zeros(X.shape[1])\n",
        "x_0_dense = x_0.todense()\n",
        "x_0.dot(w)\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "\n",
        "\n",
        "# Cross-entropy\n",
        "\n",
        "def sgd_for_lr_with_ce(X, y, num_passes=5, learning_rate = 0.1):\n",
        "\n",
        "    num_data_points = X.shape[0]\n",
        "\n",
        "    # Initialize theta -> 0\n",
        "    num_features = X.shape[1]\n",
        "    w = np.zeros(num_features)\n",
        "    b = 0.0\n",
        "\n",
        "    # repeat until done\n",
        "    # how to define \"done\"? let's just make it num passes for now\n",
        "    # we can also do norm of gradient and when it is < epsilon (something tiny)\n",
        "    # we stop\n",
        "\n",
        "    for current_pass in range(num_passes):\n",
        "        \n",
        "        # iterate through entire dataset in random order\n",
        "        order = list(range(num_data_points))\n",
        "        random.shuffle(order)\n",
        "        for i in order:\n",
        "\n",
        "            # compute y-hat for this value of i given y_i and x_i\n",
        "            x_i = X[i]\n",
        "            y_i = y[i]\n",
        "\n",
        "            # need to compute based on w and b\n",
        "            # sigmoid(w dot x + b)\n",
        "            z = x_i.dot(w) + b\n",
        "            y_hat_i = expit(z)\n",
        "\n",
        "            # for each w (and b), modify by -lr * (y_hat_i - y_i) * x_i\n",
        "            w = w - learning_rate * (y_hat_i - y_i) * x_i\n",
        "            b = b - learning_rate * (y_hat_i - y_i)\n",
        "\n",
        "    # return theta\n",
        "    return w,b\n",
        "\n",
        "w,b = sgd_for_lr_with_ce(X,y)\n",
        "\n",
        "#w\n",
        "\n",
        "sorted_vocab = sorted([(k,v) for k,v in vectorizer.vocabulary_.items()],key=lambda x:x[1])\n",
        "sorted_vocab = [a for (a,b) in sorted_vocab]\n",
        "\n",
        "sorted_words_weights = sorted([x for x in zip(sorted_vocab, w)], key=lambda x:x[1])\n",
        "sorted_words_weights[-50:]\n",
        "\n",
        "# get the predictions\n",
        "def predict_y_lr(w,b,X,threshold=0.5):\n",
        "\n",
        "    # use our matrix operation version of the logistic regression model\n",
        "    # X dot w + b\n",
        "    # need to make w a column vector so the dimensions line up correctly\n",
        "    y_hat = X.dot( w.reshape((-1,1)) ) + b\n",
        "\n",
        "    # then just check if it's > threshold\n",
        "    preds = np.where(y_hat > threshold,1,0)\n",
        "\n",
        "    return preds\n",
        "\n",
        "preds = predict_y_lr(w,b,X)\n",
        "\n",
        "preds\n",
        "\n",
        "# compute training set results\n",
        "from sklearn.metrics import classification_report\n",
        "w,b = sgd_for_lr_with_ce(X, y, num_passes=10)\n",
        "y_pred = predict_y_lr(w,b,X)\n",
        "print(classification_report(y, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "DsnU_c3QLVJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RPT82OeaL7Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "# Download and load the pretrained embedding model\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
        "!unzip crawl-300d-2M.vec.zip\n",
        "embeddings = KeyedVectors.load_word2vec_format('crawl-300d-2M.vec')\n",
        "\n"
      ],
      "metadata": {
        "id": "nymVtJxaF5st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gensim\n",
        "\n",
        "# Load the embeddings\n",
        "embeddings = gensim.models.KeyedVectors.load_word2vec_format('crawl-300d-2M.vec', binary=False)\n",
        "\n",
        "# Define your training and test data and labels\n",
        "train_texts = [...]  # list of training texts\n",
        "train_labels = [...]  # list of training labels\n",
        "test_texts = [...]  # list of test texts\n",
        "test_labels = [...]  # list of test labels\n",
        "\n",
        "# Preprocess your training and test data\n",
        "train_data = []\n",
        "for text in train_texts:\n",
        "    # Tokenize the text\n",
        "    tokens = gensim.utils.simple_preprocess(text)\n",
        "    \n",
        "    # Generate the averaged embedding for the text\n",
        "    embedding = np.mean([embeddings[token] for token in tokens if token in embeddings.vocab], axis=0)\n",
        "    \n",
        "    # Append the embedding to the training data\n",
        "    train_data.append(embedding)\n",
        "    \n",
        "# Preprocess your test data\n",
        "test_data = []\n",
        "for text in test_texts:\n",
        "    # Tokenize the text\n",
        "    tokens = gensim.utils.simple_preprocess(text)\n",
        "    \n",
        "    # Generate the averaged embedding for the text\n",
        "    embedding = np.mean([embeddings[token] for token in tokens if token in embeddings.vocab], axis=0)\n",
        "    \n",
        "    # Append the embedding to the test data\n",
        "    test_data.append(embedding)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "numa1zjsINNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define a function to preprocess the text and generate the input features\n",
        "def preprocess(texts: List[str], embeddings: KeyedVectors, dim: int) -> np.ndarray:\n",
        "    features = []\n",
        "    for text in texts:\n",
        "        tokens = text.lower().split()\n",
        "        token_vectors = []\n",
        "        for token in tokens:\n",
        "            if token in embeddings:\n",
        "                token_vectors.append(embeddings[token])\n",
        "        if token_vectors:\n",
        "            features.append(np.mean(token_vectors, axis=0))\n",
        "        else:\n",
        "            features.append(np.zeros(dim))\n",
        "    return np.array(features)\n",
        "\n",
        "# Load the data and preprocess it\n",
        "train_texts = [...]  # list of training texts\n",
        "train_labels = [...]  # list of training labels\n",
        "test_texts = [...]  # list of test texts\n",
        "test_labels = [...]  # list of test labels\n",
        "dim = 300  # dimensionality of the pretrained embeddings\n",
        "train_features = preprocess(train_texts, embeddings, dim)\n",
        "test_features = preprocess(test_texts, embeddings, dim)\n",
        "\n",
        "# Convert the data to PyTorch tensors and create datasets\n",
        "train_features_tensor = torch.from_numpy(train_features).float()\n",
        "train_labels_tensor = torch.tensor(train_labels)\n",
        "train_dataset = TensorDataset(train_features_tensor, train_labels_tensor)\n",
        "test_features_tensor = torch.from_numpy(test_features).float()\n",
        "test_labels_tensor = torch.tensor(test_labels)\n",
        "test_dataset = TensorDataset(test_features_tensor, test_labels_tensor)\n",
        "\n",
        "# Define the logistic regression model\n",
        "class LogisticRegression(torch.nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(dim, 1)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Train the logistic regression model\n",
        "lr_model = LogisticRegression(dim)\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(lr_model.parameters())\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "for epoch in range(10):\n",
        "    for i, (features, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = lr_model(features)\n",
        "        loss = criterion(outputs.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Test the logistic regression model\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for features, labels in test_loader:\n",
        "        outputs = lr_model(features)\n",
        "        predictions.extend(outputs.squeeze().tolist())\n",
        "binary_predictions = [1 if p >= 0.5 else 0 for p in predictions]\n",
        "accuracy = accuracy_score(test_labels, binary_predictions)\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "7WaokEnbIUFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/sagorbrur/covid-19-community-detection/raw/main/models/sbert_covid19_community_detection.bin\n",
        "!wget https://github.com/sagorbrur/covid-19-community-detection/raw/main/models/sbert_covid19_community_detection_config.json\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = AutoModel.from_pretrained('sbert_covid19_community_detection.bin')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load your data into a pandas dataframe with a 'text' column\n",
        "data = pd.read_csv('your_data.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = data['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "max_len = 32\n",
        "padded_text = np.array([i + [0]*(max_len-len(i)) for i in tokenized_text.values])\n",
        "\n",
        "# Generate input features from the model\n",
        "with torch.no_grad():\n",
        "    features = model(torch.LongTensor(padded_text))[0][:,0,:].numpy()\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a logistic regression classifier\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "LqjST4E4DUYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "\n",
        "!wget https://zenodo.org/record/4123773/files/covid19-community-embedding.bin\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the AG_NEWS dataset with labels\n",
        "dataset = load_dataset('ag_news', split='train[:90%]')\n",
        "\n",
        "\n",
        "import gensim\n",
        "\n",
        "\n",
        "# Load the CBOW-based pretrained embeddings\n",
        "'''tokenizer = AutoTokenizer.from_pretrained('sagorsarker/covid-19-community')\n",
        "model = AutoModel.from_pretrained('sagorsarker/covid-19-community')'''\n",
        "model_path = 'covid19-community-embedding.bin'\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
        "\n",
        "\n",
        "# Define a function to generate input features from the embeddings\n",
        "def generate_features(text):\n",
        "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
        "    outputs = model(input_ids)\n",
        "    features = outputs[0].detach().numpy()[0].mean(axis=0)\n",
        "    return features\n",
        "\n",
        "# Generate input features for each example in the dataset\n",
        "X = np.array([generate_features(example['text']) for example in dataset])\n",
        "\n",
        "# Extract the labels from the dataset\n",
        "y = np.array([example['label'] for example in dataset])\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_X, valid_X = X[:train_size], X[train_size:]\n",
        "train_y, valid_y = y[:train_size], y[train_size:]\n",
        "\n",
        "# Train a logistic regression classifier on the training set\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(train_X, train_y)\n",
        "\n",
        "# Evaluate the classifier on the validation set\n",
        "valid_preds = clf.predict(valid_X)\n",
        "valid_acc = accuracy_score(valid_y, valid_preds)\n",
        "print('Validation accuracy:', valid_acc)\n"
      ],
      "metadata": {
        "id": "4amw97ZOvpPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "# download AG_NEWS dataset with labels\n",
        "train_dataset, test_dataset = AG_NEWS(root='./data')\n",
        "\n",
        "# load pretrained CBOW word embeddings\n",
        "vectors = Vectors(name='path/to/pretrained/word2vec')\n",
        "\n",
        "# build vocabulary using pretrained embeddings\n",
        "vocab = train_dataset.get_vocab()\n",
        "vocab.set_vectors(vectors.stoi, vectors.vectors, vectors.dim)\n",
        "\n",
        "# define text and label field\n",
        "text_field = torchtext.data.Field(lower=True, tokenize='spacy', batch_first=True, fix_length=100, include_lengths=True, vocabulary=vocab)\n",
        "label_field = torchtext.data.Field(sequential=False)\n",
        "\n",
        "# apply fields to dataset\n",
        "train_data = torchtext.data.TabularDataset(path='./data/ag_news_csv/train.csv', format='csv', skip_header=True, fields=[('label', label_field), ('text', text_field)])\n",
        "test_data = torchtext.data.TabularDataset(path='./data/ag_news_csv/test.csv', format='csv', skip_header=True, fields=[('label', label_field), ('text', text_field)])\n",
        "\n",
        "# create iterators for batching and padding\n",
        "train_iter, test_iter = torchtext.data.Iterator.splits((train_data, test_data), batch_sizes=(64, 64), sort_within_batch=True, sort_key=lambda x: len(x.text), repeat=False)\n",
        "\n",
        "# define logistic regression model\n",
        "class LogisticRegression(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# define model hyperparameters\n",
        "input_dim = vectors.dim\n",
        "output_dim = len(train_dataset.get_labels())\n",
        "learning_rate = 0.01\n",
        "epochs = 10\n",
        "\n",
        "# initialize model and optimizer\n",
        "model = LogisticRegression(input_dim, output_dim)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# train model\n",
        "for epoch in range(epochs):\n",
        "    for batch in train_iter:\n",
        "        text, text_lengths = batch.text\n",
        "        labels = batch.label\n",
        "        optimizer.zero_grad()\n",
        "        output = model(text).squeeze(1)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# evaluate model performance\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, text_lengths = batch.text\n",
        "        labels = batch.label\n",
        "        output = model(text).squeeze(1)\n",
        "        predictions = torch.argmax(output, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predictions == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print('Accuracy: {:.2f}%'.format(accuracy))\n"
      ],
      "metadata": {
        "id": "Kl9wOAxtuIhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchdata torchvision torchtext torchaudio fastai\n",
        "!pip install portalocker\n",
        "!pip install --pre torch torchdata -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n"
      ],
      "metadata": {
        "id": "Paeff43SiCEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "# download and load the dataset\n",
        "train_data, test_data = AG_NEWS(root='./data')\n",
        "\n",
        "# download and load the pre-trained CBOW word embeddings\n",
        "url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip'\n",
        "vector_path = './data/crawl-300d-2M.vec'\n",
        "vectors = Vectors(name=vector_path, url=url)\n",
        "\n",
        "# build the vocabulary using the pre-trained embeddings\n",
        "text_field = torchtext.data.Field(tokenize='spacy', lower=True)\n",
        "label_field = torchtext.data.LabelField(dtype=torch.long)\n",
        "train_data, test_data = torchtext.datasets.AG_NEWS.splits(text_field=text_field, label_field=label_field)\n",
        "text_field.build_vocab(train_data, vectors=vectors)\n",
        "\n",
        "# define the input size and output size\n",
        "input_size = len(text_field.vocab)\n",
        "output_size = len(label_field.vocab)\n",
        "\n",
        "# define the logistic regression model\n",
        "class LogisticRegression(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "model = LogisticRegression(input_size, output_size)\n",
        "\n",
        "# define the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# create the training and testing dataloaders\n",
        "train_loader = torchtext.data.BucketIterator(train_data, batch_size=32, sort_key=lambda x: len(x.text), shuffle=True)\n",
        "test_loader = torchtext.data.BucketIterator(test_data, batch_size=32, sort_key=lambda x: len(x.text))\n",
        "\n",
        "# train the model\n",
        "for epoch in range(10):\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        inputs = batch.text.transpose(0, 1)\n",
        "        labels = batch.label - 1\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('Epoch {}, Batch {}, Loss {}'.format(epoch+1, i+1, loss.item()))\n",
        "\n",
        "# test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        inputs = batch.text.transpose(0, 1)\n",
        "        labels = batch.label - 1\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Test Accuracy: {}%'.format(100 * correct / total))\n"
      ],
      "metadata": {
        "id": "9hrnHVuklJdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import AG_NEWS\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_dataset, test_dataset = AG_NEWS(root='data', split=('train', 'test'))\n",
        "\n",
        "vectors = Vectors(name='glove.6B.100d.txt', cache='data')\n",
        "\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "TEXT = torchtext.legacy.data.Field(tokenize=tokenizer, include_lengths=True)\n",
        "LABEL = torchtext.legacy.data.LabelField(dtype=torch.float)\n",
        "\n",
        "TEXT.build_vocab(train_dataset, vectors=vectors, max_size=10000, unk_init=torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_dataset)\n",
        "\n",
        "vocab = TEXT.vocab\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def generate_features(dataset):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for text, label in dataset:\n",
        "        tokens = [vocab.stoi[token] for token in tokenizer(text)]\n",
        "        data.append(tokens)\n",
        "        labels.append(label)\n",
        "    return torch.tensor(data), torch.tensor(labels)\n",
        "\n",
        "train_data, train_labels = generate_features(train_dataset)\n",
        "test_data, test_labels = generate_features(test_dataset)\n",
        "\n",
        "train_loader = DataLoader(list(zip(train_data, train_labels)), batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(list(zip(test_data, test_labels)), batch_size=64, shuffle=False)\n",
        "\n",
        "model = LogisticRegression(input_size=len(vocab), output_size=4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    for data, target in loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * len(data)\n",
        "        total_correct += (output.argmax(dim=1) == target).sum().item()\n",
        "\n",
        "    return total_loss / len(loader.dataset), total_correct / len(loader.dataset)\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target.long())\n",
        "\n",
        "            total_loss += loss.item() * len(data)\n",
        "            total_correct += (output.argmax(dim=1) == target).sum().item()\n",
        "\n",
        "    return total_loss / len(loader.dataset), total_correct / len(loader.dataset)\n",
        "\n",
        "for epoch in range(10):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "\n",
        "    print('Epoch {}: train_loss={:.4f}, train_acc={:.4f}, test_loss={:.4f}, test_acc={:.4f}'.format(\n",
        "        epoch + 1, train_loss, train_acc, test_loss, test_acc))\n"
      ],
      "metadata": {
        "id": "l9masp6CiDrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install torchtext\n",
        "!pip install torchdata\n",
        "\n",
        "import torch\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from torchtext.vocab import GloVe\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 64\n",
        "\n",
        "# Load the AG_NEWS dataset with labels\n",
        "train_data, test_data = AG_NEWS(root='data', split=('train', 'test'))\n",
        "\n",
        "# Load pretrained word embeddings\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define the embedding function\n",
        "def get_embedding(text):\n",
        "    # Split the text into words\n",
        "    tokens = text.split(' ')\n",
        "    # Get the embeddings for each word\n",
        "    embeddings = [glove_vectors[token] for token in tokens]\n",
        "    # Return the mean of the embeddings\n",
        "    return torch.mean(torch.stack(embeddings), dim=0)\n",
        "\n",
        "# Define the data loader\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Generate input features for the logistic regression classifier\n",
        "X_train = []\n",
        "y_train = []\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "for data, target in train_loader:\n",
        "    # Generate embeddings for the text data\n",
        "    embeddings = [get_embedding(text) for text in data]\n",
        "    # Convert the embeddings to numpy arrays and append to X_train\n",
        "    X_train.append(torch.stack(embeddings).detach().numpy())\n",
        "    # Append the targets to y_train\n",
        "    y_train.append(target.detach().numpy())\n",
        "\n",
        "for data, target in test_loader:\n",
        "    # Generate embeddings for the text data\n",
        "    embeddings = [get_embedding(text) for text in data]\n",
        "    # Convert the embeddings to numpy arrays and append to X_test\n",
        "    X_test.append(torch.stack(embeddings).detach().numpy())\n",
        "    # Append the targets to y_test\n",
        "    y_test.append(target.detach().numpy())\n",
        "\n",
        "# Flatten the lists and convert to numpy arrays\n",
        "X_train = np.concatenate(X_train)\n",
        "y_train = np.concatenate(y_train)\n",
        "X_test = np.concatenate(X_test)\n",
        "y_test = np.concatenate(y_test)\n",
        "\n",
        "# Train a logistic regression classifier on the input features\n",
        "clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the classifier on the test set\n",
        "score = clf.score(X_test, y_test)\n",
        "print('Accuracy:', score)\n"
      ],
      "metadata": {
        "id": "ach4WsJefPQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define your word lists\n",
        "young_words = [\"youth\", \"energetic\", \"fun\", \"carefree\", \"vibrant\"]\n",
        "middle_aged_words = [\"career\", \"family\", \"stressed\", \"busy\", \"successful\"]\n",
        "elderly_words = [\"wisdom\", \"experience\", \"retired\", \"peaceful\", \"relaxed\"]\n",
        "\n",
        "# Prepare the target and attribute word sets\n",
        "target_words = [\"successful\", \"unsuccessful\"]\n",
        "attribute_words = [young_words, middle_aged_words, elderly_words]\n",
        "\n",
        "# Calculate the effect size\n",
        "def weat_effect_size(X, Y, A, B):\n",
        "    mean_X = np.mean(X, axis=0)\n",
        "    mean_Y = np.mean(Y, axis=0)\n",
        "    mean_A = np.mean(A, axis=0)\n",
        "    mean_B = np.mean(B, axis=0)\n",
        "    std_X = np.std(X, axis=0)\n",
        "    std_Y = np.std(Y, axis=0)\n",
        "    z = (mean_X - mean_Y) / np.sqrt((std_X ** 2 + std_Y ** 2) / 2)\n",
        "    numerator = np.dot(A.T, z)\n",
        "    denominator = np.dot(B.T, z)\n",
        "    return np.mean(numerator) / np.mean(denominator)\n",
        "\n",
        "X = np.array([model[w] for w in target_words])\n",
        "Y = np.concatenate([model[w] for w in young_words])\n",
        "Z = np.concatenate([model[w] for w in middle_aged_words])\n",
        "A = np.concatenate([model[w] for w in elderly_words])\n",
        "\n",
        "effect_size = weat_effect_size(X, Y, Z, A)\n",
        "\n",
        "# Interpret the results\n",
        "if effect_size > 0:\n",
        "    print(\"There is a positive age bias towards successful careers.\")\n",
        "else:\n",
        "    print(\"There is a negative age bias towards successful careers.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "DsuzqRTkWwIr",
        "outputId": "d893efa6-f097-49f1-a522-4a28bc57ca3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e10cb0bce6ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenominator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myoung_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmiddle_aged_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-e10cb0bce6ed>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenominator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myoung_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmiddle_aged_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install scipy\n",
        "\n",
        "# Political bias word sets\n",
        "setA = ['liberal', 'progressive', 'democrat', 'left-wing', 'feminist', 'gay', 'environmentalist']\n",
        "setB = ['conservative', 'traditional', 'republican', 'right-wing', 'religious', 'straight', 'capitalist']\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "glove_vectors = KeyedVectors.load_word2vec_format('path/to/glove.6B.300d.txt', binary=False)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def cos_sim(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "def weat(setA, setB, targetX, targetY, word_vectors, association_measure):\n",
        "    \"\"\"\n",
        "    Calculates the WEAT effect size and p-value for the given word sets and association measures.\n",
        "    \"\"\"\n",
        "    A = np.mean([word_vectors[w] for w in setA], axis=0)\n",
        "    B = np.mean([word_vectors[w] for w in setB], axis=0)\n",
        "    X = np.mean([word_vectors[w] for w in targetX], axis=0)\n",
        "    Y = np.mean([word_vectors[w] for w in targetY], axis=0)\n",
        "\n",
        "    effect_size = np.dot(X - Y, A - B) / np.linalg.norm(A - B)\n",
        "\n",
        "    # Calculate the standard deviation of differences for each set of words\n",
        "    setA_diff = np.array([association_measure(word_vectors[w], X - Y) for w in setA])\n",
        "    setB_diff = np.array([association_measure(word_vectors[w], X - Y) for w in setB])\n",
        "    diff = setA_diff - setB_diff\n",
        "    std_dev = np.std(diff, ddof=1)\n",
        "\n",
        "    # Calculate the t-statistic and p-value\n",
        "    t = effect_size / (std_dev / np.sqrt(len(setA)))\n",
        "    p = ttest_ind(setA_diff, setB_diff, equal_var=False)[1]\n",
        "\n",
        "    return p\n",
        "\n",
        "# Target categories\n",
        "targetX = ['good', 'excellent', 'positive', 'pleasant', 'satisfactory', 'superior']\n",
        "targetY = ['bad', 'poor', 'negative', 'unpleasant', 'unsatisfactory', 'inferior']\n",
        "\n",
        "# Calculate the p-value\n",
        "p_value = weat(setA, setB, targetX, targetY, glove\n"
      ],
      "metadata": {
        "id": "EFTFZTP1VYzm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}