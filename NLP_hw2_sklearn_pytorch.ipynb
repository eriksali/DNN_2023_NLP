{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJS/FtaRyNbOHX/Wts7mw5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eriksali/DNN_2023_NLP/blob/main/NLP_hw2_sklearn_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XncGbCl-GvOc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1.\tEmbeddings\n",
        "\n",
        "The two variations of embeddings for training are skip-gram based embeddings and CBOW based embeddings, and the library genism was used to train them. \n",
        "\n",
        "'''\n",
        "\n",
        "!pip install datasets \n",
        "!pip install apache_bea\n",
        "!pip install gensim\n",
        "!pip install fasttext\n",
        "!pip install apache_beam\n",
        "from datasets import load_dataset\n",
        "\n",
        "import gensim\n",
        "import fasttext\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the Wikipedia dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")['train']\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = [nltk.word_tokenize(text.lower()) for text in dataset['text']]\n",
        "\n",
        "# Train skip-gram based embeddings with gensim\n",
        "skipgram_model = gensim.models.Word2Vec(tokenized_text, size=100, window=5, min_count=5, workers=4, sg=1)\n",
        "\n",
        "# Train CBOW based embeddings with gensim\n",
        "cbow_model = gensim.models.Word2Vec(tokenized_text, size=100, window=5, min_count=5, workers=4, sg=0)\n",
        "\n",
        "\n",
        "# Save the models\n",
        "skipgram_model.save(\"skipgram.model\")\n",
        "cbow_model.save(\"cbow.model\")\n",
        "\n"
      ],
      "metadata": {
        "id": "exHhkLqyppxf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''!pip install torch torchvision\n",
        "!pip install datasets\n",
        "!pip install transformers'''\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the AG_NEWS dataset with labels\n",
        "dataset = load_dataset('ag_news', split='train[:90%]')\n",
        "\n",
        "# Load the CBOW-based pretrained embeddings\n",
        "tokenizer = AutoTokenizer.from_pretrained('cbow.model.pt')\n",
        "model = AutoModel.from_pretrained('cbow.model.pt')\n",
        "\n",
        "'''\n",
        "# Load the saved model\n",
        "model_path = \"/content/cbow_model.pt\"\n",
        "model = torch.load(model_path)'''\n",
        "\n",
        "\n",
        "# Define a function to generate input features from the embeddings\n",
        "def generate_features(text):\n",
        "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
        "    outputs = model(input_ids)\n",
        "    features = outputs[0].detach().numpy()[0].mean(axis=0)\n",
        "    return features\n",
        "\n",
        "# Generate input features for each example in the dataset\n",
        "X = np.array([generate_features(example['text']) for example in dataset])\n",
        "\n",
        "# Extract the labels from the dataset\n",
        "y = np.array([example['label'] for example in dataset])\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_X, valid_X = X[:train_size], X[train_size:]\n",
        "train_y, valid_y = y[:train_size], y[train_size:]\n",
        "\n",
        "# Train a logistic regression classifier on the training set\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(train_X, train_y)\n",
        "\n",
        "# Evaluate the classifier on the validation set\n",
        "valid_preds = clf.predict(valid_X)\n",
        "valid_acc = accuracy_score(valid_y, valid_preds)\n",
        "print('Validation accuracy:', valid_acc)\n"
      ],
      "metadata": {
        "id": "MEi8Hs5VitVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## 1.\tEmbeddings\n",
        "\n",
        "## The two other sets of pretrained embeddings are glove.6B.100d and word2vec-google-news-300.\n",
        "\n",
        "\n",
        "'''!pip install torch torchvision\n",
        "!pip install datasets\n",
        "!pip install transformers'''\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Load the AG_NEWS dataset with labels\n",
        "dataset = load_dataset('ag_news', split='train[:90%]')\n",
        "\n",
        "'''# Define a function to generate input features from the embeddings\n",
        "def generate_features(text):\n",
        "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
        "    outputs = model(input_ids)\n",
        "    features = outputs[0].detach().numpy()[0].mean(axis=0)\n",
        "    return features'''\n",
        "\n",
        "'''# Load the CBOW-based pretrained embeddings\n",
        "tokenizer = AutoTokenizer.from_pretrained('cbow.model')\n",
        "model = AutoModel.from_pretrained('cbow.model')'''\n",
        "\n",
        "'''# Load the saved model\n",
        "model_path = \"/content/cbow.model\"\n",
        "model = torch.load(model_path)'''\n",
        "\n",
        "model = AutoModel.from_pretrained('cbow.model')\n",
        "\n",
        "# Define a function to generate input features from the embeddings\n",
        "def generate_features(text):\n",
        "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
        "    outputs = model(input_ids)\n",
        "    features = outputs[0].detach().numpy()[0].mean(axis=0)\n",
        "    return features\n",
        "\n",
        "\n",
        "# Generate input features for each example in the dataset\n",
        "X = np.array([generate_features(example['text']) for example in dataset])\n",
        "\n",
        "# Extract the labels from the dataset\n",
        "y = np.array([example['label'] for example in dataset])\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_X, valid_X = X[:train_size], X[train_size:]\n",
        "train_y, valid_y = y[:train_size], y[train_size:]\n",
        "\n",
        "# Train a logistic regression classifier on the training set\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(train_X, train_y)\n",
        "\n",
        "# Evaluate the classifier on the validation set\n",
        "valid_preds = clf.predict(valid_X)\n",
        "valid_acc = accuracy_score(valid_y, valid_preds)\n",
        "print('Validation accuracy:', valid_acc)\n"
      ],
      "metadata": {
        "id": "xDhmeikKwb70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example queries\n",
        "print(cbow_model.most_similar('country'))\n",
        "print(cbow_model.most_similar(positive=['browser', 'firefox'], negative=['chrome']))\n",
        "print(cbow_model.most_similar(positive=['fruit', 'orange']))\n",
        "print(cbow_model.most_similar(positive=['he','him','his','himself'], negative=['she','her','hers','herself']))\n",
        "print(cbow_model.most_similar(positive=['me','my','myself'], negative=['you','your','yourself']))\n",
        "print('################################################################################################')\n",
        "print(skipgram_model.most_similar('country'))\n",
        "print(cbow_model.most_similar(positive=['browser', 'firefox'], negative=['chrome']))\n",
        "print(skipgram_model.most_similar(positive=['fruit', 'orange']))\n",
        "print(skipgram_model.most_similar(positive=['he','him','his','himself'], negative=['she','her','hers','herself']))\n",
        "print(skipgram_model.most_similar(positive=['me','my','myself'], negative=['you','your','yourself']))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z4uR_1H-LrUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "\n",
        "# Define the path to the GloVe embeddings file\n",
        "glove_path = \"glove.6B.100d.txt\"\n",
        "\n",
        "# Load the GloVe embeddings into a dictionary\n",
        "embeddings_dict = {}\n",
        "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.strip().split()\n",
        "        word = values[0]\n",
        "        vector = torch.tensor([float(val) for val in values[1:]])\n",
        "        embeddings_dict[word] = vector\n",
        "\n",
        "# Define the positive and negative words\n",
        "positive_words = ['browser', 'firefox']\n",
        "negative_words = ['chrome']\n",
        "\n",
        "# Compute the combined vector of the positive words\n",
        "positive_vectors = [embeddings_dict[word] for word in positive_words if word in embeddings_dict]\n",
        "positive_vector = torch.mean(torch.stack(positive_vectors), dim=0)\n",
        "\n",
        "# Compute the combined vector of the negative words\n",
        "negative_vectors = [embeddings_dict[word] for word in negative_words if word in embeddings_dict]\n",
        "negative_vector = torch.mean(torch.stack(negative_vectors), dim=0)\n",
        "\n",
        "# Compute the query vector as the difference between the positive and negative vectors\n",
        "query_vector = positive_vector - negative_vector\n",
        "\n",
        "# Load the list of words to preprocess\n",
        "words_to_preprocess = ['browser', 'firefox', 'chrome', 'apple', 'orange', 'fruit', 'country']\n",
        "\n",
        "# Create a mapping from words to indices\n",
        "word_to_index = {}\n",
        "for word in words_to_preprocess:\n",
        "    if word in embeddings_dict:\n",
        "        word_to_index[word] = len(word_to_index)\n",
        "\n",
        "# Create a PyTorch tensor to store the preprocessed data\n",
        "preprocessed_data = torch.zeros(len(word_to_index), len(embeddings_dict[word]))\n",
        "\n",
        "# Preprocess the data\n",
        "for word, index in word_to_index.items():\n",
        "    preprocessed_data[index] = embeddings_dict[word]\n",
        "\n",
        "# Compute the cosine similarities between the query vector and all other vectors\n",
        "similarities = {}\n",
        "for word, index in word_to_index.items():\n",
        "    embedding = preprocessed_data[index]\n",
        "    similarities[word] = torch.dot(query_vector, embedding) / (torch.norm(query_vector) * torch.norm(embedding))\n",
        "\n",
        "# Sort the similarities in descending order and print the top 10 most similar words\n",
        "sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "for word, similarity in sorted_similarities[:10]:\n",
        "    print(f\"{word}: {similarity:.3f}\")\n"
      ],
      "metadata": {
        "id": "A5CWTgPtuDJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2.\tBias\n",
        "## The word lists for age bias was extended to conduct a WEAT. \n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define the word lists for the WEAT test\n",
        "age_words = ['old', 'elderly', 'senior', 'retired', 'aged', 'elder', 'youthful', 'young', 'youth', 'teenager']\n",
        "job_words = ['doctor', 'nurse', 'teacher', 'lawyer', 'engineer', 'scientist', 'artist', 'writer', 'actor', 'musician']\n",
        "\n",
        "# Define the target and attribute word sets\n",
        "target_words = age_words\n",
        "attribute_words = job_words\n",
        "\n",
        "# Calculate the embeddings for the target and attribute words\n",
        "target_embeddings = np.array([cbow_model.wv[word] for word in target_words])\n",
        "attribute_embeddings = np.array([cbow_model.wv[word] for word in attribute_words])\n",
        "\n",
        "# Calculate the mean embeddings for the target and attribute word sets\n",
        "target_mean_embedding = np.mean(target_embeddings, axis=0)\n",
        "attribute_mean_embedding = np.mean(attribute_embeddings, axis=0)\n",
        "\n",
        "# Calculate the cosine similarities between the target and attribute word embeddings\n",
        "cos_similarities = cosine_similarity(target_embeddings, attribute_mean_embedding.reshape(1, -1))\n",
        "\n",
        "# Calculate the effect size of the WEAT test\n",
        "effect_size = np.mean(cos_similarities) / np.std(cos_similarities)\n",
        "\n",
        "# Print the effect size of the WEAT test\n",
        "print(\"Effect size:\", effect_size)\n"
      ],
      "metadata": {
        "id": "MOxSSCTjo_eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define the word lists for the WEAT test\n",
        "age_words = ['old', 'elderly', 'senior', 'retired', 'aged', 'elder', 'youthful', 'young', 'youth', 'teenager']\n",
        "job_words = ['doctor', 'nurse', 'teacher', 'lawyer', 'engineer', 'scientist', 'artist', 'writer', 'actor', 'musician']\n",
        "\n",
        "# Define the target and attribute word sets\n",
        "target_words = age_words\n",
        "attribute_words = job_words\n",
        "\n",
        "# Calculate the embeddings for the target and attribute words\n",
        "target_embeddings = np.array([skipgram_model.wv[word] for word in target_words])\n",
        "attribute_embeddings = np.array([skipgram_model.wv[word] for word in attribute_words])\n",
        "\n",
        "# Calculate the mean embeddings for the target and attribute word sets\n",
        "target_mean_embedding = np.mean(target_embeddings, axis=0)\n",
        "attribute_mean_embedding = np.mean(attribute_embeddings, axis=0)\n",
        "\n",
        "# Calculate the cosine similarities between the target and attribute word embeddings\n",
        "cos_similarities = cosine_similarity(target_embeddings, attribute_mean_embedding.reshape(1, -1))\n",
        "\n",
        "# Calculate the effect size of the WEAT test\n",
        "effect_size = np.mean(cos_similarities) / np.std(cos_similarities)\n",
        "\n",
        "# Print the effect size of the WEAT test\n",
        "print(\"Effect size:\", effect_size)\n"
      ],
      "metadata": {
        "id": "vcJmbXQSpQgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3.\tClassification\n",
        "\n",
        "## The sentiment analysis task (aclImdb_v1.tar.gz) was used to train a simple logistic regression classifier for a text classification task. \n",
        "\n",
        "## First model:\n",
        "\n",
        "## The bag-of-words features were used, the model was evaluated on a held-out test set. \n",
        "\n",
        "from sklearn.datasets import load_files\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('aclImdb_v1.csv')\n",
        "\n",
        "# Convert reviews to lowercase\n",
        "reviews = data['review'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
        "\n",
        "# Tokenize the reviews\n",
        "reviews = reviews.apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])\n",
        "\n",
        "# Get the labels\n",
        "labels = data['sentiment'].values\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the text data into bag-of-words features\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform([' '.join(review) for review in X_train])\n",
        "X_test = vectorizer.transform([' '.join(review) for review in X_test])\n",
        "\n",
        "# Train the logistic regression model\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "bAosS7opuZn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3.\tClassification\n",
        "\n",
        "## The sentiment analysis task (aclImdb_v1.tar.gz) was used to train a simple logistic regression classifier for a text classification task. \n",
        "\n",
        "## First model:\n",
        "\n",
        "## The bag-of-words features were used, the model was evaluated on a held-out test set. \n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.nn import functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('aclImdb_v1.csv')\n",
        "\n",
        "# Convert reviews to lowercase\n",
        "reviews = data['review'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize the reviews\n",
        "reviews = reviews.apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])\n",
        "\n",
        "# Get the labels\n",
        "labels = data['sentiment'].values\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the text data into bag-of-words features\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform([' '.join(review) for review in X_train])\n",
        "X_test = vectorizer.transform([' '.join(review) for review in X_test])\n",
        "\n",
        "# Convert data to PyTorch Tensors\n",
        "X_train = torch.tensor(X_train.toarray(), dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test.toarray(), dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create PyTorch DataLoader for train and test data\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Define logistic regression model\n",
        "class LogisticRegressionModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = LogisticRegressionModel(X_train.shape[1], 2)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Define training and evaluation functions\n",
        "def train(model, optimizer, train_loader):\n",
        "    model.train()\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_x)\n",
        "        loss = F.cross_entropy(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            output = model(batch_x)\n",
        "            test_loss += F.cross_entropy(output, batch_y, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(batch_y.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = correct / len(test_loader.dataset)\n",
        "    return test_loss, accuracy\n",
        "\n",
        "# Train and evaluate model\n",
        "for epoch in range(10):\n",
        "    train(model, optimizer, train_loader)\n",
        "    test_loss, accuracy = evaluate(model, test_loader)\n",
        "    print(f'Epoch {epoch+1}, Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for batch_x, _ in test_loader:\n",
        "        output = model(batch_x)\n",
        "        pred = output\n",
        "y_pred.extend(torch.argmax(pred, dim=1).tolist())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKlNx4UmuKJl",
        "outputId": "6ad5e828-242e-46dc-ef67-3f8f2860b203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Second model:\n",
        "\n",
        "## The GloVe embeddings glove.6B.100d to generate the input features. \n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "# Load the GloVe embeddings\n",
        "glove = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('aclImdb_v1.csv')\n",
        "reviews = data['review'].values\n",
        "labels = data['sentiment'].values\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_reviews, test_reviews, train_labels, test_labels = train_test_split(reviews, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the reviews to embeddings\n",
        "'''def get_embedding(text):\n",
        "    tokens = text.lower().split()\n",
        "    embedding = np.zeros((100,))\n",
        "    \n",
        "    count = 0\n",
        "    for token in tokens:\n",
        "        if token in glove.stoi:\n",
        "            embedding = np.concatenate((embedding, glove.vectors[glove.stoi[token]]))\n",
        "            count += 1\n",
        "    if count != 0:\n",
        "        embedding = embedding.sum(axis=0) / count\n",
        "    return embedding'''\n",
        "def get_embedding(text):\n",
        "    tokens = text.lower().split()\n",
        "    embeddings = []\n",
        "    for token in tokens:\n",
        "        if token in glove.stoi:\n",
        "            embeddings.append(glove.vectors[glove.stoi[token]])\n",
        "    if embeddings:\n",
        "        embeddings = np.stack(embeddings)\n",
        "        embedding = embeddings.mean(axis=0)\n",
        "    else:\n",
        "        embedding = np.zeros((glove.dim,))\n",
        "    return embedding\n",
        "\n",
        "train_embeddings = np.array([get_embedding(text) for text in train_reviews])\n",
        "test_embeddings = np.array([get_embedding(text) for text in test_reviews])\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "train_embeddings = torch.tensor(train_embeddings, dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "test_embeddings = torch.tensor(test_embeddings, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "# Define the logistic regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# Initialize the model and the loss function\n",
        "model = LogisticRegression(100, 2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "total_steps = len(train_embeddings) // batch_size\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(total_steps):\n",
        "        batch_embeddings = train_embeddings[i*batch_size:(i+1)*batch_size]\n",
        "        batch_labels = train_labels[i*batch_size:(i+1)*batch_size]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_embeddings)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the model\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(test_embeddings)\n",
        "    test_predictions = torch.argmax(test_outputs, dim=1)\n",
        "    accuracy = accuracy_score(test_labels, test_predictions)\n",
        "    f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_labels, test_predictions))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZWTOEt9SuhRR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}