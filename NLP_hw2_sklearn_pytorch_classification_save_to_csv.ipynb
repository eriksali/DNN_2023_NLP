{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiVJirN1y/Xw1HrEQ5Y/BO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eriksali/DNN_2023_NLP/blob/main/NLP_hw2_sklearn_pytorch_classification_save_to_csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XncGbCl-GvOc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1.\tEmbeddings\n",
        "\n",
        "The two variations of embeddings for training are skip-gram based embeddings and CBOW based embeddings, and the library genism was used to train them. \n",
        "\n",
        "'''\n",
        "\n",
        "!pip install datasets \n",
        "!pip install apache_bea\n",
        "!pip install gensim\n",
        "!pip install fasttext\n",
        "!pip install apache_beam\n",
        "from datasets import load_dataset\n",
        "\n",
        "import gensim\n",
        "import fasttext\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the Wikipedia dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")['train']\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = [nltk.word_tokenize(text.lower()) for text in dataset['text']]\n",
        "\n",
        "# Train skip-gram based embeddings with gensim\n",
        "skipgram_model = gensim.models.Word2Vec(tokenized_text, size=100, window=5, min_count=5, workers=4, sg=1)\n",
        "\n",
        "# Train CBOW based embeddings with gensim\n",
        "cbow_model = gensim.models.Word2Vec(tokenized_text, size=100, window=5, min_count=5, workers=4, sg=0)\n",
        "\n",
        "\n",
        "# Save the models\n",
        "skipgram_model.save(\"skipgram.model\")\n",
        "cbow_model.save(\"cbow.model\")\n",
        "\n"
      ],
      "metadata": {
        "id": "exHhkLqyppxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''!pip install torch torchvision\n",
        "!pip install datasets\n",
        "!pip install transformers'''\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the AG_NEWS dataset with labels\n",
        "dataset = load_dataset('ag_news', split='train[:90%]')\n",
        "\n",
        "# Load the CBOW-based pretrained embeddings\n",
        "tokenizer = AutoTokenizer.from_pretrained('cbow.model.pt')\n",
        "model = AutoModel.from_pretrained('cbow.model.pt')\n",
        "\n",
        "'''\n",
        "# Load the saved model\n",
        "model_path = \"/content/cbow_model.pt\"\n",
        "model = torch.load(model_path)'''\n",
        "\n",
        "\n",
        "# Define a function to generate input features from the embeddings\n",
        "def generate_features(text):\n",
        "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
        "    outputs = model(input_ids)\n",
        "    features = outputs[0].detach().numpy()[0].mean(axis=0)\n",
        "    return features\n",
        "\n",
        "# Generate input features for each example in the dataset\n",
        "X = np.array([generate_features(example['text']) for example in dataset])\n",
        "\n",
        "# Extract the labels from the dataset\n",
        "y = np.array([example['label'] for example in dataset])\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_X, valid_X = X[:train_size], X[train_size:]\n",
        "train_y, valid_y = y[:train_size], y[train_size:]\n",
        "\n",
        "# Train a logistic regression classifier on the training set\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(train_X, train_y)\n",
        "\n",
        "# Evaluate the classifier on the validation set\n",
        "valid_preds = clf.predict(valid_X)\n",
        "valid_acc = accuracy_score(valid_y, valid_preds)\n",
        "print('Validation accuracy:', valid_acc)\n"
      ],
      "metadata": {
        "id": "MEi8Hs5VitVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## 1.\tEmbeddings\n",
        "\n",
        "## The two other sets of pretrained embeddings are glove.6B.100d and word2vec-google-news-300.\n",
        "\n",
        "\n",
        "'''!pip install torch torchvision\n",
        "!pip install datasets\n",
        "!pip install transformers'''\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Load the AG_NEWS dataset with labels\n",
        "dataset = load_dataset('ag_news', split='train[:90%]')\n",
        "\n",
        "'''# Define a function to generate input features from the embeddings\n",
        "def generate_features(text):\n",
        "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
        "    outputs = model(input_ids)\n",
        "    features = outputs[0].detach().numpy()[0].mean(axis=0)\n",
        "    return features'''\n",
        "\n",
        "'''# Load the CBOW-based pretrained embeddings\n",
        "tokenizer = AutoTokenizer.from_pretrained('cbow.model')\n",
        "model = AutoModel.from_pretrained('cbow.model')'''\n",
        "\n",
        "'''# Load the saved model\n",
        "model_path = \"/content/cbow.model\"\n",
        "model = torch.load(model_path)'''\n",
        "\n",
        "model = AutoModel.from_pretrained('cbow.model')\n",
        "\n",
        "# Define a function to generate input features from the embeddings\n",
        "def generate_features(text):\n",
        "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
        "    outputs = model(input_ids)\n",
        "    features = outputs[0].detach().numpy()[0].mean(axis=0)\n",
        "    return features\n",
        "\n",
        "\n",
        "# Generate input features for each example in the dataset\n",
        "X = np.array([generate_features(example['text']) for example in dataset])\n",
        "\n",
        "# Extract the labels from the dataset\n",
        "y = np.array([example['label'] for example in dataset])\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_X, valid_X = X[:train_size], X[train_size:]\n",
        "train_y, valid_y = y[:train_size], y[train_size:]\n",
        "\n",
        "# Train a logistic regression classifier on the training set\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(train_X, train_y)\n",
        "\n",
        "# Evaluate the classifier on the validation set\n",
        "valid_preds = clf.predict(valid_X)\n",
        "valid_acc = accuracy_score(valid_y, valid_preds)\n",
        "print('Validation accuracy:', valid_acc)\n"
      ],
      "metadata": {
        "id": "xDhmeikKwb70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example queries\n",
        "print(cbow_model.most_similar('country'))\n",
        "print(cbow_model.most_similar(positive=['browser', 'firefox'], negative=['chrome']))\n",
        "print(cbow_model.most_similar(positive=['fruit', 'orange']))\n",
        "print(cbow_model.most_similar(positive=['he','him','his','himself'], negative=['she','her','hers','herself']))\n",
        "print(cbow_model.most_similar(positive=['me','my','myself'], negative=['you','your','yourself']))\n",
        "print('################################################################################################')\n",
        "print(skipgram_model.most_similar('country'))\n",
        "print(cbow_model.most_similar(positive=['browser', 'firefox'], negative=['chrome']))\n",
        "print(skipgram_model.most_similar(positive=['fruit', 'orange']))\n",
        "print(skipgram_model.most_similar(positive=['he','him','his','himself'], negative=['she','her','hers','herself']))\n",
        "print(skipgram_model.most_similar(positive=['me','my','myself'], negative=['you','your','yourself']))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z4uR_1H-LrUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "\n",
        "# Define the path to the GloVe embeddings file\n",
        "glove_path = \"glove.6B.100d.txt\"\n",
        "\n",
        "# Load the GloVe embeddings into a dictionary\n",
        "embeddings_dict = {}\n",
        "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.strip().split()\n",
        "        word = values[0]\n",
        "        vector = torch.tensor([float(val) for val in values[1:]])\n",
        "        embeddings_dict[word] = vector\n",
        "\n",
        "# Define the positive and negative words\n",
        "positive_words = ['browser', 'firefox']\n",
        "negative_words = ['chrome']\n",
        "\n",
        "# Compute the combined vector of the positive words\n",
        "positive_vectors = [embeddings_dict[word] for word in positive_words if word in embeddings_dict]\n",
        "positive_vector = torch.mean(torch.stack(positive_vectors), dim=0)\n",
        "\n",
        "# Compute the combined vector of the negative words\n",
        "negative_vectors = [embeddings_dict[word] for word in negative_words if word in embeddings_dict]\n",
        "negative_vector = torch.mean(torch.stack(negative_vectors), dim=0)\n",
        "\n",
        "# Compute the query vector as the difference between the positive and negative vectors\n",
        "query_vector = positive_vector - negative_vector\n",
        "\n",
        "# Load the list of words to preprocess\n",
        "words_to_preprocess = ['browser', 'firefox', 'chrome', 'apple', 'orange', 'fruit', 'country']\n",
        "\n",
        "# Create a mapping from words to indices\n",
        "word_to_index = {}\n",
        "for word in words_to_preprocess:\n",
        "    if word in embeddings_dict:\n",
        "        word_to_index[word] = len(word_to_index)\n",
        "\n",
        "# Create a PyTorch tensor to store the preprocessed data\n",
        "preprocessed_data = torch.zeros(len(word_to_index), len(embeddings_dict[word]))\n",
        "\n",
        "# Preprocess the data\n",
        "for word, index in word_to_index.items():\n",
        "    preprocessed_data[index] = embeddings_dict[word]\n",
        "\n",
        "# Compute the cosine similarities between the query vector and all other vectors\n",
        "similarities = {}\n",
        "for word, index in word_to_index.items():\n",
        "    embedding = preprocessed_data[index]\n",
        "    similarities[word] = torch.dot(query_vector, embedding) / (torch.norm(query_vector) * torch.norm(embedding))\n",
        "\n",
        "# Sort the similarities in descending order and print the top 10 most similar words\n",
        "sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "for word, similarity in sorted_similarities[:10]:\n",
        "    print(f\"{word}: {similarity:.3f}\")\n"
      ],
      "metadata": {
        "id": "A5CWTgPtuDJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2.\tBias\n",
        "## The word lists for age bias was extended to conduct a WEAT. \n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define the word lists for the WEAT test\n",
        "age_words = ['old', 'elderly', 'senior', 'retired', 'aged', 'elder', 'youthful', 'young', 'youth', 'teenager']\n",
        "job_words = ['doctor', 'nurse', 'teacher', 'lawyer', 'engineer', 'scientist', 'artist', 'writer', 'actor', 'musician']\n",
        "\n",
        "# Define the target and attribute word sets\n",
        "target_words = age_words\n",
        "attribute_words = job_words\n",
        "\n",
        "# Calculate the embeddings for the target and attribute words\n",
        "target_embeddings = np.array([cbow_model.wv[word] for word in target_words])\n",
        "attribute_embeddings = np.array([cbow_model.wv[word] for word in attribute_words])\n",
        "\n",
        "# Calculate the mean embeddings for the target and attribute word sets\n",
        "target_mean_embedding = np.mean(target_embeddings, axis=0)\n",
        "attribute_mean_embedding = np.mean(attribute_embeddings, axis=0)\n",
        "\n",
        "# Calculate the cosine similarities between the target and attribute word embeddings\n",
        "cos_similarities = cosine_similarity(target_embeddings, attribute_mean_embedding.reshape(1, -1))\n",
        "\n",
        "# Calculate the effect size of the WEAT test\n",
        "effect_size = np.mean(cos_similarities) / np.std(cos_similarities)\n",
        "\n",
        "# Print the effect size of the WEAT test\n",
        "print(\"Effect size:\", effect_size)\n"
      ],
      "metadata": {
        "id": "MOxSSCTjo_eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define the word lists for the WEAT test\n",
        "age_words = ['old', 'elderly', 'senior', 'retired', 'aged', 'elder', 'youthful', 'young', 'youth', 'teenager']\n",
        "job_words = ['doctor', 'nurse', 'teacher', 'lawyer', 'engineer', 'scientist', 'artist', 'writer', 'actor', 'musician']\n",
        "\n",
        "# Define the target and attribute word sets\n",
        "target_words = age_words\n",
        "attribute_words = job_words\n",
        "\n",
        "# Calculate the embeddings for the target and attribute words\n",
        "target_embeddings = np.array([skipgram_model.wv[word] for word in target_words])\n",
        "attribute_embeddings = np.array([skipgram_model.wv[word] for word in attribute_words])\n",
        "\n",
        "# Calculate the mean embeddings for the target and attribute word sets\n",
        "target_mean_embedding = np.mean(target_embeddings, axis=0)\n",
        "attribute_mean_embedding = np.mean(attribute_embeddings, axis=0)\n",
        "\n",
        "# Calculate the cosine similarities between the target and attribute word embeddings\n",
        "cos_similarities = cosine_similarity(target_embeddings, attribute_mean_embedding.reshape(1, -1))\n",
        "\n",
        "# Calculate the effect size of the WEAT test\n",
        "effect_size = np.mean(cos_similarities) / np.std(cos_similarities)\n",
        "\n",
        "# Print the effect size of the WEAT test\n",
        "print(\"Effect size:\", effect_size)\n"
      ],
      "metadata": {
        "id": "vcJmbXQSpQgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3.\tClassification\n",
        "\n",
        "## The sentiment analysis task (aclImdb_v1.tar.gz) was used to train a simple logistic regression classifier for a text classification task. \n",
        "\n",
        "## First model:\n",
        "\n",
        "## The bag-of-words features were used, the model was evaluated on a held-out test set. \n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"aclImdb_v1_2000.csv\")\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Extract the bag-of-words features from the text data\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_data[\"review\"])\n",
        "y_train = train_data[\"sentiment\"].values\n",
        "X_test = vectorizer.transform(test_data[\"review\"])\n",
        "y_test = test_data[\"sentiment\"].values\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_train = torch.from_numpy(X_train.toarray()).float()\n",
        "y_train = torch.from_numpy(y_train).long()\n",
        "X_test = torch.from_numpy(X_test.toarray()).float()\n",
        "y_test = torch.from_numpy(y_test).long()\n",
        "\n",
        "# Define the logistic regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(num_features, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# Set the hyperparameters\n",
        "num_features = X_train.shape[1]\n",
        "num_classes = 2\n",
        "lr = 0.01\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = LogisticRegression(num_features, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "        # Extract the batch\n",
        "        inputs = X_train[i:i+batch_size]\n",
        "        labels = y_train[i:i+batch_size]\n",
        "        \n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    # Print the loss after each epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    accuracy = accuracy_score(y_test, predicted.numpy())\n",
        "    f1 = f1_score(y_test, predicted.numpy(), average=\"macro\")\n",
        "\n",
        "# Print the evaluation metrics\n",
        "##print(f\"Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, predicted.numpy()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZdcmPVZMYey",
        "outputId": "197b1587-80b9-42b0-fd2b-7006bd433107"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.9475\n",
            "Epoch [2/10], Loss: 0.6455\n",
            "Epoch [3/10], Loss: 0.4267\n",
            "Epoch [4/10], Loss: 0.3748\n",
            "Epoch [5/10], Loss: 0.3505\n",
            "Epoch [6/10], Loss: 0.3270\n",
            "Epoch [7/10], Loss: 0.3037\n",
            "Epoch [8/10], Loss: 0.2855\n",
            "Epoch [9/10], Loss: 0.2730\n",
            "Epoch [10/10], Loss: 0.2632\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.75      0.79       201\n",
            "           1       0.77      0.85      0.81       199\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.81      0.80      0.80       400\n",
            "weighted avg       0.81      0.80      0.80       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Second model:\n",
        "\n",
        "## The GloVe embeddings glove.6B.100d to generate the input features. \n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "# Load the GloVe embeddings\n",
        "glove = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('aclImdb_v1_2000.csv')\n",
        "reviews = data['review'].values\n",
        "labels = data['sentiment'].values\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_reviews, test_reviews, train_labels, test_labels = train_test_split(reviews, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the reviews to embeddings\n",
        "def get_embedding(text):\n",
        "    tokens = text.lower().split()\n",
        "    embeddings = []\n",
        "    for token in tokens:\n",
        "        if token in glove.stoi:\n",
        "            embeddings.append(glove.vectors[glove.stoi[token]])\n",
        "    if embeddings:\n",
        "        embeddings = np.stack(embeddings)\n",
        "        embedding = embeddings.mean(axis=0)\n",
        "    else:\n",
        "        embedding = np.zeros((glove.dim,))\n",
        "    return embedding\n",
        "\n",
        "train_embeddings = np.array([get_embedding(text) for text in train_reviews])\n",
        "test_embeddings = np.array([get_embedding(text) for text in test_reviews])\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "train_embeddings = torch.tensor(train_embeddings, dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "test_embeddings = torch.tensor(test_embeddings, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "# Define the logistic regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# Initialize the model and the loss function\n",
        "model = LogisticRegression(100, 2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "total_steps = len(train_embeddings) // batch_size\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(total_steps):\n",
        "        batch_embeddings = train_embeddings[i*batch_size:(i+1)*batch_size]\n",
        "        batch_labels = train_labels[i*batch_size:(i+1)*batch_size]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_embeddings)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the model\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(test_embeddings)\n",
        "    test_predictions = torch.argmax(test_outputs, dim=1)\n",
        "    accuracy = accuracy_score(test_labels, test_predictions)\n",
        "    f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_labels, test_predictions))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPsFZ22hN8r0",
        "outputId": "8b92eec7-6117-4dad-ad7b-0b77e96474e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.38MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:16<00:00, 24747.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.68      0.67       201\n",
            "           1       0.67      0.67      0.67       199\n",
            "\n",
            "    accuracy                           0.67       400\n",
            "   macro avg       0.67      0.67      0.67       400\n",
            "weighted avg       0.67      0.67      0.67       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Save the DataFrame to CSV\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "# Download the dataset\n",
        "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "filename = 'aclImdb_v1.tar.gz'\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# Extract the dataset\n",
        "with tarfile.open(filename, 'r:gz') as tar:\n",
        "    tar.extractall()\n",
        "\n",
        "# Create a Pandas DataFrame from the dataset\n",
        "rows = []\n",
        "labels = {'pos': 1, 'neg': 0}\n",
        "for split in ['train', 'test']:\n",
        "    for label in ['pos', 'neg']:\n",
        "        folder = f'aclImdb/{split}/{label}'\n",
        "        for filename in os.listdir(folder):\n",
        "            with open(os.path.join(folder, filename), 'r') as file:\n",
        "                review = file.read()\n",
        "            rows.append({'review': review, 'sentiment': labels[label]})\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Save the DataFrame to CSV\n",
        "df.to_csv('aclImdb_v1.csv', index=False)"
      ],
      "metadata": {
        "id": "kVOU8YrlUJAV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}