{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1odzFaGNczl/TH8DayPp4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eriksali/DNN_2023_NLP/blob/main/Copy_of_NLP_hw2_fastText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XncGbCl-GvOc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets \n",
        "!pip install apache_bea\n",
        "!pip install gensim\n",
        "!pip install fasttext\n",
        "!pip install apache_beam\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
        "# check the first example of the training portion of the dataset:\n",
        "print(dataset['train'][0])\n",
        "\n",
        "import gensim\n",
        "import fasttext\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the Wikipedia dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")['train']\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = [nltk.word_tokenize(text.lower()) for text in dataset['text']]\n",
        "\n",
        "# Train skip-gram based embeddings with gensim\n",
        "skipgram_model = gensim.models.Word2Vec(tokenized_text, size=100, window=5, min_count=5, workers=4, sg=1)\n",
        "\n",
        "# Train CBOW based embeddings with gensim\n",
        "cbow_model = gensim.models.Word2Vec(tokenized_text, size=100, window=5, min_count=5, workers=4, sg=0)\n",
        "\n",
        "\n",
        "# Save the models\n",
        "skipgram_model.save(\"skipgram.model\")\n",
        "cbow_model.save(\"cbow.model\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zPlDtyU608iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# build vocabulary and train model\n",
        "    model = gensim.models.Word2Vec(\n",
        "        documents,\n",
        "        size=150,\n",
        "        window=10,\n",
        "        min_count=2,\n",
        "        workers=10,\n",
        "        iter=10)\n",
        "'''\n",
        "###############################################################################\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the Wikipedia dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")['train']\n",
        "\n",
        "# Save the dataset to a text file\n",
        "with open(\"data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for data in dataset:\n",
        "        f.write(data[\"text\"].lower() + \"\\n\")\n",
        "\n",
        "!pip install fasttext\n",
        "import fasttext\n",
        "\n",
        "# Train skip-gram based embeddings with fasttext\n",
        "skipgram_model = fasttext.train_unsupervised(\"data.txt\", model=\"skipgram\")\n",
        "\n",
        "# Train CBOW based embeddings with fasttext\n",
        "cbow_model = fasttext.train_unsupervised(\"data.txt\", model=\"cbow\")\n",
        "\n",
        "# Save the models\n",
        "skipgram_model.save_model(\"skipgram.bin\")\n",
        "cbow_model.save_model(\"cbow.bin\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_U-FVHxs1F2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dataset to a text file\n",
        "with open(\"data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for data in dataset:\n",
        "        f.write(data[\"text\"].lower() + \"\\n\")\n",
        "        \n",
        "# !zip -r reviews_yellow_mountain.zip reviews_yellow_mountain/\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XFRilH8cWO07"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip data.zip data.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIrAcZFbWdal",
        "outputId": "885a3e92-f5df-4423-c6dd-dd4715e7e322"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: data.txt (deflated 65%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets \n",
        "!pip install apache_bea\n",
        "!pip install gensim\n",
        "!pip install fasttext\n",
        "!pip install apache_beam\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
        "# check the first example of the training portion of the dataset:\n",
        "print(dataset['train'][0])\n",
        "\n",
        "import gensim\n",
        "import fasttext\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the Wikipedia dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")['train']\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = [nltk.word_tokenize(text.lower()) for text in dataset['text']]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7i6SRVKXG2jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets \n",
        "!pip install apache_bea\n",
        "!pip install gensim\n",
        "!pip install fasttext\n",
        "!pip install apache_beam\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
        "# check the first example of the training portion of the dataset:\n",
        "print(dataset['train'][0])\n",
        "\n",
        "import gensim\n",
        "import fasttext\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the Wikipedia dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")['train']\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = [nltk.word_tokenize(text.lower()) for text in dataset['text']]\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "import fasttext\n",
        "import os\n",
        "\n",
        "# Load the Wikipedia dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")['train']\n",
        "\n",
        "# Save the dataset to a text file\n",
        "with open(\"data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for example in dataset:\n",
        "        f.write(example[\"text\"].lower() + \"\\n\")\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
        "# check the first example of the training portion of the dataset:\n",
        "print(dataset['train'][0])\n",
        "\n",
        "import gensim\n",
        "import fasttext\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = [nltk.word_tokenize(text.lower()) for text in dataset['text']]\n",
        "\n",
        "\n",
        "# Train skip-gram based embeddings with fasttext\n",
        "skipgram_model = fasttext.train_unsupervised(\"data.txt\", model=\"skipgram\")\n",
        "\n",
        "# Train CBOW based embeddings with fasttext\n",
        "cbow_model = fasttext.train_unsupervised(\"data.txt\", model=\"cbow\")\n",
        "\n",
        "# Train skip-gram based embeddings with fasttext\n",
        "fasttext_model = fasttext.train_unsupervised('data.txt', model='skipgram', dim=100, epoch=10, lr=0.1, minCount=5)\n",
        "\n",
        "# Train CBOW based embeddings with fasttext\n",
        "fasttext_model_cbow = fasttext.train_unsupervised('data.txt', model='cbow', dim=100, epoch=10, lr=0.1, minCount=5)\n",
        "\n",
        "\n",
        "# Save the models\n",
        "skipgram_model.save(\"skipgram.model\")\n",
        "cbow_model.save(\"cbow.model\")\n",
        "fasttext_model.save_model(\"fasttext_skipgram.bin\")\n",
        "fasttext_model_cbow.save_model(\"fasttext_cbow.bin\")"
      ],
      "metadata": {
        "id": "bZVnJY49RHgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ofRIoKpvHjyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets \n",
        "!pip install apache_bea\n",
        "!pip install gensim\n",
        "!pip install fasttext\n",
        "!pip install apache_beam\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
        "# check the first example of the training portion of the dataset:\n",
        "print(dataset['train'][0])\n",
        "\n",
        "import gensim\n",
        "import fasttext\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the Wikipedia dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")['train']\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = [nltk.word_tokenize(text.lower()) for text in dataset['text']]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AwY0PaIBhKkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install datasets \n",
        "!pip install apache_bea\n",
        "\n",
        "!pip install apache_beam\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
        "# check the first example of the training portion of the dataset:\n",
        "print(dataset['train'][0])\n",
        "\n",
        "\n",
        "\n",
        "################################################################\n",
        "! pip install --upgrade gensim\n",
        "import gensim\n",
        "gensim.__version__\n",
        "\n",
        "# download pretrained embeddings\n",
        "\n",
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')\n",
        "\n",
        "for index, word in enumerate(wv.index_to_key):\n",
        "    if index == 10:\n",
        "        break\n",
        "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")\n",
        "\n",
        "\n",
        "vec_king = wv['king']\n",
        "print(vec_king)\n",
        "\n",
        "pairs = [\n",
        "    ('car', 'minivan'),   # a minivan is a kind of car\n",
        "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
        "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
        "    ('car', 'cereal'),    # ... and so on\n",
        "    ('car', 'communism'),\n",
        "]\n",
        "for w1, w2 in pairs:\n",
        "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))\n",
        "\n",
        "\n",
        "print(wv.most_similar(positive=['car', 'minivan'], topn=5))\n",
        "print(wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=5))\n",
        "print(wv.n_similarity( \"I was at the store\".split(), \"You did some shopping\".split()))\n",
        "print(wv.n_similarity( \"I was at the store\".split(), \"She ate an apple\".split()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uo5M9SOSB6fN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}